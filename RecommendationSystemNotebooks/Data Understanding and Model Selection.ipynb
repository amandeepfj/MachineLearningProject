{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding, Model Selection, and Parameter Optimization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing we will do is to load and explore the data.\n",
    "<br> - The data is loaded from SQLite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from sqllite database which has all the csv files dumped earlier.\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "conn = sqlite3.connect(\"C:\\Users\\Amandeep\\pluralsight.db\")\n",
    "\n",
    "user_assessment_scores = pd.read_sql_query(\"select * from user_assessment_scores;\", conn)\n",
    "user_course_views = pd.read_sql_query(\"select * from user_course_views;\", conn)\n",
    "course_tags = pd.read_sql_query(\"select * from course_tags;\", conn)\n",
    "user_interests = pd.read_sql_query(\"select * from user_interests;\", conn)\n",
    "\n",
    "# Close connection of sqlite. Always remember to close the connection!\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view how the data looks like. Normally the data has problems either on it's head or tail! We will see the 4 head and 4 tail rows of each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Assessments : Dims = (6571, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_handle</th>\n",
       "      <th>assessment_tag</th>\n",
       "      <th>user_assessment_date</th>\n",
       "      <th>user_assessment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7487</td>\n",
       "      <td>angular-js</td>\n",
       "      <td>2017-08-11 19:03:38</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7487</td>\n",
       "      <td>css</td>\n",
       "      <td>2017-08-11 20:09:56</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7487</td>\n",
       "      <td>html5</td>\n",
       "      <td>2017-07-31 18:59:37</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7487</td>\n",
       "      <td>java</td>\n",
       "      <td>2017-07-31 18:49:27</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_handle assessment_tag user_assessment_date user_assessment_score\n",
       "0        7487     angular-js  2017-08-11 19:03:38                   134\n",
       "1        7487            css  2017-08-11 20:09:56                    38\n",
       "2        7487          html5  2017-07-31 18:59:37                    84\n",
       "3        7487           java  2017-07-31 18:49:27                   149"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_handle</th>\n",
       "      <th>assessment_tag</th>\n",
       "      <th>user_assessment_date</th>\n",
       "      <th>user_assessment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6567</th>\n",
       "      <td>8887</td>\n",
       "      <td>angular-js</td>\n",
       "      <td>2016-09-30 22:30:48</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6568</th>\n",
       "      <td>8887</td>\n",
       "      <td>docker</td>\n",
       "      <td>2017-03-24 17:55:06</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6569</th>\n",
       "      <td>8887</td>\n",
       "      <td>html5</td>\n",
       "      <td>2017-02-10 16:38:53</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6570</th>\n",
       "      <td>4440</td>\n",
       "      <td>c-sharp</td>\n",
       "      <td>2017-09-04 15:58:48</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_handle assessment_tag user_assessment_date user_assessment_score\n",
       "6567        8887     angular-js  2016-09-30 22:30:48                   221\n",
       "6568        8887         docker  2017-03-24 17:55:06                   148\n",
       "6569        8887          html5  2017-02-10 16:38:53                   241\n",
       "6570        4440        c-sharp  2017-09-04 15:58:48                   126"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User Course Views : Dims = (249238, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_handle</th>\n",
       "      <th>view_date</th>\n",
       "      <th>course_id</th>\n",
       "      <th>author_handle</th>\n",
       "      <th>level</th>\n",
       "      <th>view_time_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-06-27</td>\n",
       "      <td>cpt-sp2010-web-designers-branding-intro</td>\n",
       "      <td>875</td>\n",
       "      <td>Beginner</td>\n",
       "      <td>3786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-06-28</td>\n",
       "      <td>cpt-sp2010-web-designers-branding-intro</td>\n",
       "      <td>875</td>\n",
       "      <td>Beginner</td>\n",
       "      <td>1098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-06-28</td>\n",
       "      <td>cpt-sp2010-web-designers-css</td>\n",
       "      <td>875</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>4406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-07-27</td>\n",
       "      <td>cpt-sp2010-web-designers-css</td>\n",
       "      <td>875</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_handle   view_date                                course_id  \\\n",
       "0           1  2017-06-27  cpt-sp2010-web-designers-branding-intro   \n",
       "1           1  2017-06-28  cpt-sp2010-web-designers-branding-intro   \n",
       "2           1  2017-06-28             cpt-sp2010-web-designers-css   \n",
       "3           1  2017-07-27             cpt-sp2010-web-designers-css   \n",
       "\n",
       "  author_handle         level view_time_seconds  \n",
       "0           875      Beginner              3786  \n",
       "1           875      Beginner              1098  \n",
       "2           875  Intermediate              4406  \n",
       "3           875  Intermediate               553  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_handle</th>\n",
       "      <th>view_date</th>\n",
       "      <th>course_id</th>\n",
       "      <th>author_handle</th>\n",
       "      <th>level</th>\n",
       "      <th>view_time_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>249234</th>\n",
       "      <td>9999</td>\n",
       "      <td>2017-10-14</td>\n",
       "      <td>python-getting-started</td>\n",
       "      <td>104</td>\n",
       "      <td>Beginner</td>\n",
       "      <td>1612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249235</th>\n",
       "      <td>10000</td>\n",
       "      <td>2017-08-18</td>\n",
       "      <td>apex-absolute-beginner-guide-coding-salesforce</td>\n",
       "      <td>229</td>\n",
       "      <td>Beginner</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249236</th>\n",
       "      <td>10000</td>\n",
       "      <td>2017-08-21</td>\n",
       "      <td>apex-absolute-beginner-guide-coding-salesforce</td>\n",
       "      <td>229</td>\n",
       "      <td>Beginner</td>\n",
       "      <td>9500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249237</th>\n",
       "      <td>10000</td>\n",
       "      <td>2017-08-21</td>\n",
       "      <td>apex-fundamental-coding</td>\n",
       "      <td>229</td>\n",
       "      <td>Beginner</td>\n",
       "      <td>1659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_handle   view_date  \\\n",
       "249234        9999  2017-10-14   \n",
       "249235       10000  2017-08-18   \n",
       "249236       10000  2017-08-21   \n",
       "249237       10000  2017-08-21   \n",
       "\n",
       "                                             course_id author_handle  \\\n",
       "249234                          python-getting-started           104   \n",
       "249235  apex-absolute-beginner-guide-coding-salesforce           229   \n",
       "249236  apex-absolute-beginner-guide-coding-salesforce           229   \n",
       "249237                         apex-fundamental-coding           229   \n",
       "\n",
       "           level view_time_seconds  \n",
       "249234  Beginner              1612  \n",
       "249235  Beginner               402  \n",
       "249236  Beginner              9500  \n",
       "249237  Beginner              1659  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Course Tags : Dims = (11337, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_id</th>\n",
       "      <th>course_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12-principles-animation-toon-boom-harmony-1475</td>\n",
       "      <td>2d-animation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2d-racing-game-series-unity-5-1312</td>\n",
       "      <td>game-design</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2d-racing-games-unity-volume-2-1286</td>\n",
       "      <td>game-art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2d-racing-games-unity-volume-2-1286</td>\n",
       "      <td>digital-painting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        course_id       course_tags\n",
       "0  12-principles-animation-toon-boom-harmony-1475      2d-animation\n",
       "1              2d-racing-game-series-unity-5-1312       game-design\n",
       "2             2d-racing-games-unity-volume-2-1286          game-art\n",
       "3             2d-racing-games-unity-volume-2-1286  digital-painting"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_id</th>\n",
       "      <th>course_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11333</th>\n",
       "      <td>z-os-mainframe-introduction</td>\n",
       "      <td>mainframe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11334</th>\n",
       "      <td>z-os-tso-ispf-environment-introduction</td>\n",
       "      <td>mainframe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11335</th>\n",
       "      <td>zsphere-modeling-zbrush-3505</td>\n",
       "      <td>3d-modeling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11336</th>\n",
       "      <td>zsphere-modeling-zbrush-3505</td>\n",
       "      <td>3d-sculpting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    course_id   course_tags\n",
       "11333             z-os-mainframe-introduction     mainframe\n",
       "11334  z-os-tso-ispf-environment-introduction     mainframe\n",
       "11335            zsphere-modeling-zbrush-3505   3d-modeling\n",
       "11336            zsphere-modeling-zbrush-3505  3d-sculpting"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User Interests : Dims = (297526, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_handle</th>\n",
       "      <th>interest_tag</th>\n",
       "      <th>date_followed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>mvc-scaffolding</td>\n",
       "      <td>2017-06-27 16:26:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>mvc2</td>\n",
       "      <td>2017-06-27 16:26:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>mvc-html-helpers</td>\n",
       "      <td>2017-06-27 16:26:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>mvc4-ioc</td>\n",
       "      <td>2017-06-27 16:26:52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_handle      interest_tag        date_followed\n",
       "0           1   mvc-scaffolding  2017-06-27 16:26:52\n",
       "1           1              mvc2  2017-06-27 16:26:52\n",
       "2           1  mvc-html-helpers  2017-06-27 16:26:52\n",
       "3           1          mvc4-ioc  2017-06-27 16:26:52"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_handle</th>\n",
       "      <th>interest_tag</th>\n",
       "      <th>date_followed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>297522</th>\n",
       "      <td>10000</td>\n",
       "      <td>salesforce</td>\n",
       "      <td>2017-08-14 14:56:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297523</th>\n",
       "      <td>10000</td>\n",
       "      <td>sql</td>\n",
       "      <td>2017-08-14 14:56:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297524</th>\n",
       "      <td>10000</td>\n",
       "      <td>java</td>\n",
       "      <td>2017-08-14 14:56:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297525</th>\n",
       "      <td>10000</td>\n",
       "      <td>c#</td>\n",
       "      <td>2017-08-14 14:56:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_handle interest_tag        date_followed\n",
       "297522       10000   salesforce  2017-08-14 14:56:57\n",
       "297523       10000          sql  2017-08-14 14:56:57\n",
       "297524       10000         java  2017-08-14 14:56:57\n",
       "297525       10000           c#  2017-08-14 14:56:57"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "print(\"User Assessments : Dims = \" + str(user_assessment_scores.shape))\n",
    "display(user_assessment_scores.head(4))\n",
    "display(user_assessment_scores.tail(4))\n",
    "\n",
    "print(\"\\nUser Course Views : Dims = \" + str(user_course_views.shape))\n",
    "display(user_course_views.head(4))\n",
    "display(user_course_views.tail(4))\n",
    "\n",
    "print(\"\\nCourse Tags : Dims = \" + str(course_tags.shape))\n",
    "display(course_tags.head(4))\n",
    "display(course_tags.tail(4))\n",
    "\n",
    "print(\"\\nUser Interests : Dims = \" + str(user_interests.shape))\n",
    "display(user_interests.head(4))\n",
    "display(user_interests.tail(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see if we have any missing values in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_handle              0\n",
      "assessment_tag           0\n",
      "user_assessment_date     0\n",
      "user_assessment_score    0\n",
      "dtype: int64\n",
      "user_handle          0\n",
      "view_date            0\n",
      "course_id            0\n",
      "author_handle        0\n",
      "level                0\n",
      "view_time_seconds    0\n",
      "dtype: int64\n",
      "course_id      0\n",
      "course_tags    0\n",
      "dtype: int64\n",
      "user_handle      0\n",
      "interest_tag     0\n",
      "date_followed    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(user_assessment_scores.isna().sum() + user_assessment_scores.isnull().sum())\n",
    "print(user_course_views.isnull().sum() + user_course_views.isna().sum())\n",
    "print(course_tags.isnull().sum() + course_tags.isna().sum())\n",
    "print(user_interests.isnull().sum() + user_interests.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, There are no missing values. Next step will be checking the datatypes and casting to correct datatypes if we need to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Before = \n",
      "user_handle              object\n",
      "assessment_tag           object\n",
      "user_assessment_date     object\n",
      "user_assessment_score    object\n",
      "dtype: object\n",
      "**After = \n",
      "user_handle                      object\n",
      "assessment_tag                   object\n",
      "user_assessment_date     datetime64[ns]\n",
      "user_assessment_score             int64\n",
      "dtype: object\n",
      "\n",
      "**Before = \n",
      "user_handle          object\n",
      "view_date            object\n",
      "course_id            object\n",
      "author_handle        object\n",
      "level                object\n",
      "view_time_seconds    object\n",
      "dtype: object\n",
      "**After = \n",
      "user_handle                  object\n",
      "view_date            datetime64[ns]\n",
      "course_id                    object\n",
      "author_handle                object\n",
      "level                        object\n",
      "view_time_seconds             int64\n",
      "dtype: object\n",
      "\n",
      "**Before = \n",
      "user_handle      object\n",
      "interest_tag     object\n",
      "date_followed    object\n",
      "dtype: object\n",
      "**After = \n",
      "user_handle              object\n",
      "interest_tag             object\n",
      "date_followed    datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Lets update the datatypes.\n",
    "print(\"**Before = \\n\" + str(user_assessment_scores.dtypes))\n",
    "user_assessment_scores['user_assessment_score'] = pd.to_numeric(user_assessment_scores['user_assessment_score'])\n",
    "user_assessment_scores['user_assessment_date'] = pd.to_datetime(user_assessment_scores['user_assessment_score'])\n",
    "print(\"**After = \\n\" + str(user_assessment_scores.dtypes))\n",
    "\n",
    "print(\"\\n**Before = \\n\" + str(user_course_views.dtypes))\n",
    "user_course_views['view_time_seconds'] = pd.to_numeric(user_course_views['view_time_seconds'])\n",
    "user_course_views['view_date'] = pd.to_datetime(user_course_views['view_date'])\n",
    "print(\"**After = \\n\" + str(user_course_views.dtypes))\n",
    "\n",
    "print(\"\\n**Before = \\n\" + str(user_interests.dtypes))\n",
    "user_interests['date_followed'] = pd.to_datetime(user_interests['date_followed'])\n",
    "print(\"**After = \\n\" + str(user_interests.dtypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks pretty much clean! *I am happy!* :) <br>\n",
    "There is no missing data, hence we don't have to deal with any missing data cases for now.\n",
    "<br>- Lets get rid of duplicate rows from the data. <br>- Next lets see if when we have all the data for users in interest and course view table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have some rows which are duplicate. Lets get rid of those rows, \n",
    "# so that we can train our model only once on one training set.\n",
    "\n",
    "if user_assessment_scores.drop_duplicates().shape[0] != user_assessment_scores.shape[0]:\n",
    "    user_assessment_scores = user_assessment_scores.drop_duplicates()\n",
    "    print(\"Duplicates in user assessment scores df removed\")\n",
    "    \n",
    "if user_course_views.drop_duplicates().shape[0] != user_course_views.shape[0]:\n",
    "    user_course_views = user_course_views.drop_duplicates()\n",
    "    print(\"Duplicates in user course views df removed\")\n",
    "\n",
    "if course_tags.drop_duplicates().shape[0] != course_tags.shape[0]:\n",
    "    course_tags = course_tags.drop_duplicates()\n",
    "    print(\"Duplicates in course tags df removed\")\n",
    "\n",
    "if user_interests.drop_duplicates().shape[0] != user_interests.shape[0]:\n",
    "    user_interests = user_interests.drop_duplicates()\n",
    "    print(\"Duplicates in user interest df removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let us collect all user ids. We will use this all users df to find users who have missing interests, course views, or assessments!<br> Similarly, we will also find the courses with missing tags but has user course view!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    7487\n",
      "6    7407\n",
      "7    3600\n",
      "8    8152\n",
      "9    3637\n",
      "Name: user_handle, dtype: object\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "unique_users_in_assessment = user_assessment_scores['user_handle'].drop_duplicates()\n",
    "unique_users_in_course_view = user_course_views['user_handle'].drop_duplicates()\n",
    "unique_users_interests = user_interests['user_handle'].drop_duplicates()\n",
    "\n",
    "unique_users = pd.concat([unique_users_in_assessment, unique_users_interests, unique_users_in_course_view]).drop_duplicates()\n",
    "print(unique_users.head())\n",
    "print(unique_users.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           cpt-sp2010-web-designers-branding-intro\n",
      "2                      cpt-sp2010-web-designers-css\n",
      "4    aws-certified-solutions-architect-professional\n",
      "5              aws-certified-sysops-admin-associate\n",
      "6                     aws-system-admin-fundamentals\n",
      "Name: course_id, dtype: object\n",
      "5942\n"
     ]
    }
   ],
   "source": [
    "unique_courses_in_course_view = user_course_views['course_id'].drop_duplicates()\n",
    "unique_courses_in_course_tags = course_tags['course_id'].drop_duplicates()\n",
    "unique_courses = pd.concat([unique_courses_in_course_view, unique_courses_in_course_tags]).drop_duplicates()\n",
    "print(unique_courses.head())\n",
    "print(unique_courses.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have found that there are 10000 users and 5942 courses in the system. Lets also grab unique course tags, interest tags, and assessment tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        2d-animation\n",
      "1         game-design\n",
      "2            game-art\n",
      "3    digital-painting\n",
      "4       image-editing\n",
      "Name: course_tags, dtype: object\n",
      "999\n",
      "0    angular-js\n",
      "1           css\n",
      "2         html5\n",
      "3          java\n",
      "4    javascript\n",
      "Name: assessment_tag, dtype: object\n",
      "54\n",
      "0     mvc-scaffolding\n",
      "1                mvc2\n",
      "2    mvc-html-helpers\n",
      "3            mvc4-ioc\n",
      "4         mvc-testing\n",
      "Name: interest_tag, dtype: object\n",
      "748\n",
      "Total number of tags : 1191\n"
     ]
    }
   ],
   "source": [
    "unqiue_course_tags = course_tags['course_tags'].drop_duplicates()\n",
    "print(unqiue_course_tags.head())\n",
    "print(unqiue_course_tags.shape[0])\n",
    "\n",
    "unqiue_assessment_tags = user_assessment_scores['assessment_tag'].drop_duplicates()\n",
    "print(unqiue_assessment_tags.head())\n",
    "print(unqiue_assessment_tags.shape[0])\n",
    "\n",
    "unique_interest_tags = user_interests['interest_tag'].drop_duplicates()\n",
    "print(unique_interest_tags.head())\n",
    "print(unique_interest_tags.shape[0])\n",
    "\n",
    "all_tags = pd.concat([unqiue_course_tags, unqiue_assessment_tags, unique_interest_tags]).drop_duplicates()\n",
    "print(\"Total number of tags : \" + str(all_tags.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results show that we have 999 unique course tags, 54 assessment tags, and 748 interest tags! The total tags are 1191 which means we have some tags common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_handle                      object\n",
       "assessment_tag                   object\n",
       "user_assessment_date     datetime64[ns]\n",
       "user_assessment_score             int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['patch.force_edgecolor']=True\n",
    "user_assessment_scores.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x184f3278>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the assessment scores\n",
    "sns.distplot(user_assessment_scores['user_assessment_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x6e29dd8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the view times\n",
    "sns.distplot(user_course_views['view_time_seconds'])\n",
    "\n",
    "# The view times are not normally distributed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot((user_course_views['view_time_seconds'])**0.2)\n",
    "# let us try to normalzie the 'view_time_seconds' by transforming. \n",
    "# the normalization will not affect KNN but help in SVD method that is used later.\n",
    "user_course_views['view_time_seconds'] = user_course_views['view_time_seconds'] ** 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Selection and Paramenter Optimization\n",
    "<br> In this section we will deal with selecting different measures we will be using.\n",
    "We are trying to find following similarities- \n",
    "**<br>- Similarity among users based on their assessments.\n",
    "<br>- Similarity among users based on their interest tags.\n",
    "<br>- Similarity among users based on time(in seconds) they spend on each course-tag.\n",
    "<br>- Similarity among users based on time(in seconds) they spend on each course.\n",
    "<br>- Similarity among users based on the level of the courses they take and spend time on each of such courses\n",
    "<br>- Finally, use all the above similarities to calculate one single similarity measure**\n",
    "\n",
    "The similarity measures for each similarities are different but the scale of similarity are going to be same.(-1 to 1) with 1 means very similar. and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from surprise.model_selection import train_test_split\\n\\nreader = Reader(rating_scale=(0, 300))\\ndata = Dataset.load_from_df(user_assessment_scores[['user_handle', 'assessment_tag', 'user_assessment_score']], reader)\\n\\nbenchmark = []\\nsim_options = {'name': 'pearson_baseline'}\\nfor algorithm in [KNNBaseline(sim_options = sim_options), KNNBasic(sim_options = sim_options), KNNWithMeans(), KNNWithZScore()]:\\n    # Perform cross validation\\n    results = cross_validate(algorithm, data, measures=['RMSE'], cv = 10, verbose=False)\\n\\n    # Get results & append algorithm name\\n    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\\n    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\\n    benchmark.append(tmp)\\n\\nreader = Reader(rating_scale=(0, 300))\\ndata = Dataset.load_from_df(user_assessment_scores[['user_handle', 'assessment_tag', 'user_assessment_score']], reader)\\n\\n# sample random trainset and testset\\n# test set is made of 25% of the ratings.\\ntrainset, testset = train_test_split(data, test_size=.25)\\n\\n# We'll use the famous SVD algorithm.\\nsim_options = {'name': 'pearson_baseline', 'user_based': False}\\nalgo = KNNWithMeans()\\n\\n# Train the algorithm on the trainset, and predict ratings for the testset\\nalgo.fit(trainset)\\npredictions = algo.test(testset)\\n\\n# Then compute RMSE\\naccuracy.rmse(predictions)\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######### KEEP THIS CELL FOR REFERENCE ######################\n",
    "\"\"\"from surprise.model_selection import train_test_split\n",
    "\n",
    "reader = Reader(rating_scale=(0, 300))\n",
    "data = Dataset.load_from_df(user_assessment_scores[['user_handle', 'assessment_tag', 'user_assessment_score']], reader)\n",
    "\n",
    "benchmark = []\n",
    "sim_options = {'name': 'pearson_baseline'}\n",
    "for algorithm in [KNNBaseline(sim_options = sim_options), KNNBasic(sim_options = sim_options), KNNWithMeans(), KNNWithZScore()]:\n",
    "    # Perform cross validation\n",
    "    results = cross_validate(algorithm, data, measures=['RMSE'], cv = 10, verbose=False)\n",
    "\n",
    "    # Get results & append algorithm name\n",
    "    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
    "    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n",
    "    benchmark.append(tmp)\n",
    "\n",
    "reader = Reader(rating_scale=(0, 300))\n",
    "data = Dataset.load_from_df(user_assessment_scores[['user_handle', 'assessment_tag', 'user_assessment_score']], reader)\n",
    "\n",
    "# sample random trainset and testset\n",
    "# test set is made of 25% of the ratings.\n",
    "trainset, testset = train_test_split(data, test_size=.25)\n",
    "\n",
    "# We'll use the famous SVD algorithm.\n",
    "sim_options = {'name': 'pearson_baseline', 'user_based': False}\n",
    "algo = KNNWithMeans()\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using 10 fold cross validation for finding optmial KNN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNBaseline, KNNBasic, KNNWithMeans, KNNWithZScore\n",
    "from surprise import Reader \n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "def findOptimalKNNAlgorithm(dataframe, user_column, item_column, score_column):\n",
    "    \"\"\"This method takes in a dataframe and the columns of user, item, and score. \n",
    "        This method will optimize for different KNN algorithms by using 10 fold CV and prints the results.\"\"\"\n",
    "    \n",
    "    reader = Reader(rating_scale=(dataframe[score_column].min(), dataframe[score_column].max()))\n",
    "    data = Dataset.load_from_df(dataframe[[user_column, item_column, score_column]], reader)\n",
    "    \n",
    "    benchmark = []\n",
    "    \n",
    "    # we use pearson baseline beacuse unlike other meausures it does not consider only common values.\n",
    "    # https://surprise.readthedocs.io/en/stable/similarities.html#surprise.similarities.pearson_baseline\n",
    "    sim_options = {'name': 'pearson_baseline'}\n",
    "    \n",
    "    # We are only using KNN algorithm here because similarity measure and getting neighbors is used by only these algorithms\n",
    "    algorithms = [KNNBaseline(sim_options = sim_options), KNNBasic(sim_options = sim_options), \n",
    "                  KNNWithMeans(), KNNWithZScore()]\n",
    "    for algorithm in algorithms:\n",
    "        # Perform cross validation. Number of folds = 10. We are using RMSE as performance measure\n",
    "        results = cross_validate(algorithm, data, measures=['RMSE'], cv = 10, verbose=False)\n",
    "\n",
    "        # Get results & append algorithm name\n",
    "        tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
    "        tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n",
    "        benchmark.append(tmp)\n",
    "    print(pd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will find optimal values for each of the similarities we are finding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#findOptimalKNNAlgorithm(user_assessment_scores, 'user_handle', 'assessment_tag', 'user_assessment_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_user_interest_df = user_interests[['user_handle', 'interest_tag']].drop_duplicates()\n",
    "cleaned_user_interest_df['interest_val'] = 1\n",
    "\n",
    "#findOptimalKNNAlgorithm(cleaned_user_interest_df, 'user_handle', 'interest_tag', 'interest_val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell takes time to run. Upto 20 Mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_courses_merge = pd.merge(left = user_course_views, right = course_tags, on = ['course_id'])\n",
    "grouped_users_course_tags = user_courses_merge[['user_handle', 'course_tags', 'view_time_seconds']]\\\n",
    "                                .groupby(['user_handle', 'course_tags'])\n",
    "meaned_group_user_courses_tag = grouped_users_course_tags.agg('mean').reset_index()\n",
    "#findOptimalKNNAlgorithm(meaned_group_user_courses_tag, 'user_handle', 'course_tags', 'view_time_seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell takes time to run. Upto 20 Mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_users_courses = user_course_views[['user_handle', 'course_id', 'view_time_seconds']]\\\n",
    "                                .groupby(['user_handle', 'course_id'])\n",
    "meaned_group_users_courses = grouped_users_courses.agg('mean').reset_index()\n",
    "#findOptimalKNNAlgorithm(meaned_group_users_courses, 'user_handle', 'course_id', 'view_time_seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNNBaseline algorithm is best suited for all the similarities. We will use KNNBaseline and it's neighbors to get the smilar users in our data. Next let us try to optimize for the value of 'K' for our SVD similarity. The bigger values of K will result to slower predictions whereas small 'K' value will result to faster but less accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def find_optimize_value_of_k_SVD(dataframe, index_column, columns_column, values_column):\n",
    "    \"\"\"Optmizes the dataframe for a value of K which is the number of dimensions that will \n",
    "    explain most of the features without any loss\"\"\"\n",
    "    scores_matrix = dataframe.pivot(index = index_column, columns = columns_column, values = values_column)\n",
    "    scores_matrix = scores_matrix.fillna(0)\n",
    "    index_values = scores_matrix.index.values\n",
    "    # we need reindexed to calclulate the cosine similarities\n",
    "    reindexed_scores_matrix = scores_matrix.copy()\n",
    "    reindexed_scores_matrix.index = range(0, index_values.shape[0])\n",
    "\n",
    "    scores_mean = np.asarray([(np.mean(reindexed_scores_matrix, 1))]).T\n",
    "    normalised_mat = reindexed_scores_matrix - scores_mean\n",
    "    A = normalised_mat.T #/ np.sqrt(self.__reindexed_scores_matrix.shape[0] - 1)\n",
    "\n",
    "    # Using svd\n",
    "    U, S, V = np.linalg.svd(A, full_matrices = False)\n",
    "\n",
    "    dim_size = 1\n",
    "    k_vs_rmse = []\n",
    "    \n",
    "    # lets set max dim size to check for optmization.\n",
    "    max_dim_size = min(scores_mean.shape[0], 2000)\n",
    "    for dim_size in range(1, max_dim_size, 2):\n",
    "        # re_create and check MSE to see if it worked!\n",
    "        S_k = np.diag(S[:dim_size])\n",
    "        U_k = U[:, :dim_size]\n",
    "        V_k = V[:dim_size, ]\n",
    "        A_k = np.dot(np.dot(U_k, S_k), V_k) + scores_mean.T\n",
    "        rmse = np.sqrt(mean_squared_error(A, A_k))\n",
    "        k_vs_rmse.append([dim_size, rmse])\n",
    "    scores_df = pd.DataFrame(k_vs_rmse, columns = ['K', 'RMSE'])\n",
    "\n",
    "    return scores_df\n",
    "\n",
    "\n",
    "def get_knee_point(scores_df):\n",
    "    \"\"\"Returns a knee point\"\"\"\n",
    "    from kneed import KneeLocator\n",
    "    kn = KneeLocator(scores_df.iloc[:, 0], scores_df.iloc[:, 1], curve='convex', direction='decreasing')\n",
    "    return kn.knee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores_df_ua = find_optimize_value_of_k_SVD(user_assessment_scores, 'user_handle', \n",
    "#                                            'assessment_tag', 'user_assessment_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.lineplot(x = 'K', y = 'RMSE', data=scores_df_ua[:100])\n",
    "#OPTIMIZED_DIM_SIZE_USER_ASSESSMENT = get_knee_point(scores_df_ua)\n",
    "#print(OPTIMIZED_DIM_SIZE_USER_ASSESSMENT)\n",
    "OPTIMIZED_DIM_SIZE_USER_ASSESSMENT = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#scores_df_ui = find_optimize_value_of_k_SVD(cleaned_user_interest_df, 'user_handle', 'interest_tag', 'interest_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.lineplot(x = 'K', y = 'RMSE', data=scores_df_ui)\n",
    "#OPTIMIZED_DIM_SIZE_USER_INTEREST = get_knee_point(scores_df_ui)\n",
    "#print(OPTIMIZED_DIM_SIZE_USER_INTEREST)\n",
    "# 191 doesn't look like a elbow point setting it to 35\n",
    "OPTIMIZED_DIM_SIZE_USER_INTEREST = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores_df_uct = find_optimize_value_of_k_SVD(meaned_group_user_courses_tag, 'user_handle', \n",
    "#                                             'course_tags', 'view_time_seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.lineplot(x = 'K', y = 'RMSE', data=scores_df_uct[:])\n",
    "#OPTIMIZED_DIM_SIZE_USER_COURSE_TAG = get_knee_point(scores_df_uct)\n",
    "#print(OPTIMIZED_DIM_SIZE_USER_COURSE_TAG)\n",
    "# the algo is giving wrong elbow setting it manually to \n",
    "OPTIMIZED_DIM_SIZE_USER_COURSE_TAG = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#grouped_users_courses = user_course_views[['user_handle', 'course_id', 'view_time_seconds']]\\\n",
    "#                                .groupby(['user_handle', 'course_id'])\n",
    "#meaned_group_users_courses = grouped_users_courses.agg('mean').reset_index()\n",
    "#scores_df_uc = find_optimize_value_of_k_SVD(meaned_group_users_courses, 'user_handle', 'course_id', 'view_time_seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.lineplot(x = 'K', y = 'RMSE', data=scores_df_uc)\n",
    "#OPTIMIZED_DIM_SIZE_USER_COURSE = get_knee_point(scores_df_uc)\n",
    "#print(OPTIMIZED_DIM_SIZE_USER_COURSE)\n",
    "# Setting K optimized value to 120 manually\n",
    "OPTIMIZED_DIM_SIZE_USER_COURSE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouped_users_course_level = user_course_views[['user_handle', 'level', 'view_time_seconds']]\\\n",
    "#                                .groupby(['user_handle', 'level'])\n",
    "#meaned_group_users_course_level = grouped_users_course_level.agg('mean').reset_index()\n",
    "#scores_df_ucl = find_optimize_value_of_k_SVD(meaned_group_users_course_level, 'user_handle', 'level', 'view_time_seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.lineplot(x = 'K', y = 'RMSE', data=scores_df_ucl)\n",
    "#OPTIMIZED_DIM_SIZE_USER_COURSE_LEVEL = get_knee_point(scores_df_ucl)\n",
    "#print(OPTIMIZED_DIM_SIZE_USER_COURSE_LEVEL)\n",
    "OPTIMIZED_DIM_SIZE_USER_COURSE_LEVEL = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model and Training Building :\n",
    "We are trying to find following similarities-\n",
    "**<br>- Similarity among users based on their assessments.\n",
    "<br>- Similarity among users based on their interest tags.\n",
    "<br>- Similarity among users based on time(in seconds) they spend on each course-tag.\n",
    "<br>- Similarity among users based on time(in seconds) they spend on each course.\n",
    "<br>- Similarity among users based on the level of the courses they take and spend time on each of such courses\n",
    "<br>- Finally, use all the above similarities to calculate one single similarity measure**\n",
    "<br><br>The optimized values found for each algorithm are as below.\n",
    "<br>- KNNBaseline Algorithm for all the similarities above.\n",
    "<br>- Cosine similarity for all the similarities above.\n",
    "<br>- SVD similarity for all the similarities above. With dimesion magnitude optimized for each of the similarities.\n",
    "<br>- We can also find pearson similarity but it is time consuming.(Heavy on model training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_CUTOFF_USERS = 10\n",
    "\n",
    "# Class for similarity measure.\n",
    "from scipy import sparse\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from surprise import Dataset\n",
    "\n",
    "class SimilarityMeasure:\n",
    "    def __initialize_matrix(self):\n",
    "        if self.__values_column is None:\n",
    "            self.__dataframe['temp_val'] = 1\n",
    "            self.__values_column = 'temp_val'\n",
    "        self.__scores_matrix = self.__dataframe.pivot(index = self.__index_column, columns= self.__columns_column, \n",
    "                                            values = self.__values_column)\n",
    "        self.__scores_matrix = self.__scores_matrix.fillna(0)\n",
    "        self.index_values = self.__scores_matrix.index.values\n",
    "        # we need reindexed to calclulate the cosine similarities\n",
    "        self.__reindexed_scores_matrix = self.__scores_matrix.copy()\n",
    "        self.__reindexed_scores_matrix.index = range(0, self.index_values.shape[0])\n",
    "        self.pearson_similarity_martix = None\n",
    "        self.cosine_similarities_matrix = None\n",
    "        self.__sliced = None\n",
    "        self.__knn_algo = None\n",
    "    \n",
    "    def calculate_pearson_similarity(self):\n",
    "        self.pearson_similarity_martix = self.__scores_matrix.T.corr(method = 'pearson')\n",
    "        print(\"Pearson similarity calculated!\")\n",
    "        \n",
    "    # Function that taken in user handle as input and outputs most similar users based on pearson.\n",
    "    def get_pearson_similar_users(self, user_handle, num_similar_users = TOP_CUTOFF_USERS):\n",
    "        if not self.isValidUser(user_handle):\n",
    "            print(\"Error - User not found!!\")\n",
    "            return None\n",
    "        if self.pearson_similarity_martix is not None:\n",
    "            user_handle_scores = pd.DataFrame(self.pearson_similarity_martix[user_handle])\n",
    "            user_handle_scores.columns = [0]\n",
    "            similar_users = user_handle_scores.sort_values(by=[0], ascending = False)[1:num_similar_users]\n",
    "            # Normalize between 0 to 1\n",
    "            similar_users[0] = (similar_users[0] - min(similar_users[0])) / (max(similar_users[0]) - min(similar_users[0]))\n",
    "            return similar_users\n",
    "        else :\n",
    "            print(\"Error - Pearson similarity not calculated!!!!\")\n",
    "    \n",
    "    # Function to calculate cosine similarity.\n",
    "    def calculate_cosine_similarity(self):\n",
    "        A_sparse = sparse.csr_matrix(self.__reindexed_scores_matrix)\n",
    "        self.cosine_similarities_matrix = cosine_similarity(A_sparse, dense_output = False)\n",
    "        print(\"Cosine similarity calculated!\")\n",
    "    \n",
    "    # Function that taken in user handle as input and outputs most similar users based on pearson.\n",
    "    def get_cosine_similar_users(self, user_handle, num_similar_users = TOP_CUTOFF_USERS):\n",
    "        if not self.isValidUser(user_handle):\n",
    "            print(\"Error - User not found!!\")\n",
    "            return None\n",
    "        if self.cosine_similarities_matrix is not None:\n",
    "            idx = np.where(self.index_values == user_handle)\n",
    "            scores = pd.DataFrame(self.cosine_similarities_matrix[idx].T.toarray(), index = self.index_values)\n",
    "            similar_users = scores.sort_values(by=[0], ascending = False)[1:num_similar_users]\n",
    "            # Normalize between 0 to 1\n",
    "            #similar_users[0] = (similar_users[0] - min(similar_users[0])) / (max(similar_users[0]) - min(similar_users[0]))\n",
    "            #similar_users = similar_users.fillna(0)\n",
    "            return similar_users\n",
    "        else :\n",
    "            print(\"Error - Cosine similarity not caclculated!!!!\")\n",
    "        \n",
    "    def calculate_svd_similarity(self, full_matrix = False, dim_size = 20):\n",
    "        scores_mean = np.asarray([(np.mean(self.__reindexed_scores_matrix, 1))]).T\n",
    "        normalised_mat = self.__reindexed_scores_matrix - scores_mean\n",
    "        A = normalised_mat.T\n",
    "        # Using svd\n",
    "        U, S, V = np.linalg.svd(A, full_matrices = full_matrix)\n",
    "        #Reducing the dimensions\n",
    "        self.__sliced = V.T[:, :dim_size]\n",
    "        # special matrix multiplication to get the magnitudes. \n",
    "        # element wise multiplication and summation row1 * col1, row2 * col2...so on\n",
    "        self.__magnitude = np.sqrt(np.einsum('ij, ij -> i', self.__sliced, self.__sliced))\n",
    "        print(\"SVD similarity calculated!\")\n",
    "\n",
    "        \n",
    "    def get_svd_similar_users(self, user_handle, num_similar_users = TOP_CUTOFF_USERS):\n",
    "        if not self.isValidUser(user_handle):\n",
    "            print(\"Error - User not found!!\")\n",
    "            return None\n",
    "        if self.__sliced is not None:\n",
    "            index = np.where(self.index_values == user_handle)[0][0] # we need index as int not array\n",
    "            user_row = self.__sliced[index, :]\n",
    "            similarity = np.dot(user_row, self.__sliced.T) / (self.__magnitude[index] * self.__magnitude)\n",
    "            scores = pd.DataFrame(similarity, index = self.index_values)\n",
    "            similar_users = scores.sort_values(by=[0], ascending = False)[1:num_similar_users]\n",
    "            # Normalize between 0 to 1\n",
    "            #similar_users[0] = (similar_users[0] - min(similar_users[0])) / (max(similar_users[0]) - min(similar_users[0]))\n",
    "            return similar_users\n",
    "        else :\n",
    "            print(\"Error - SVD Similarity not calculated!!!!\")\n",
    "    \n",
    "    \n",
    "    # Function that taken in user handle as input and outputs most similar users based on pearson.\n",
    "    def train_KNN_BaseLine(self):\n",
    "        reader = Reader(rating_scale=(self.__dataframe[self.__values_column].min(), \n",
    "                                      self.__dataframe[self.__values_column].max()))\n",
    "        data = Dataset.load_from_df(self.__dataframe[[self.__index_column, self.__columns_column, \n",
    "                                                      self.__values_column]], reader) \n",
    "        sim_options = {'name': 'pearson_baseline'}\n",
    "        self.__knn_algo = KNNWithMeans(sim_options)\n",
    "        # Train the algorithm on the trainset, and predict ratings for the testset\n",
    "        self.__knn_algo.fit(data.build_full_trainset())\n",
    "    \n",
    "    def get_KNN_similar_users(self, user_handle, num_similar_users = TOP_CUTOFF_USERS):\n",
    "        if not self.isValidUser(user_handle):\n",
    "            print(\"Error - User not found!!\")\n",
    "            return None\n",
    "        if self.__knn_algo is not None:\n",
    "            index = np.where(self.index_values == user_handle)[0][0] # we need index as int not array\n",
    "            index_neighbor_users = self.__knn_algo.get_neighbors(index, k = num_similar_users)\n",
    "            \n",
    "            scores = pd.DataFrame(self.__knn_algo.sim[index].T, index = self.index_values)\n",
    "            similar_users = scores.sort_values(by=[0], ascending = False)[1:num_similar_users]\n",
    "            # Normalize between 0 to 1\n",
    "            #similar_users[0] = (similar_users[0] - min(similar_users[0])) / (max(similar_users[0]) - min(similar_users[0]))\n",
    "            return similar_users\n",
    "        else :\n",
    "            print(\"Error - KNN Similarity not calculated!!!!\")\n",
    "    \n",
    "    def isValidUser(self, user_handle):\n",
    "        return ((self.index_values == user_handle).sum() > 0)\n",
    "    \n",
    "    def __init__(self, parameters):\n",
    "        self.__dataframe = parameters['dataframe'].copy()\n",
    "        self.__index_column = parameters['index_column']\n",
    "        # The column in dataframe which will be used to created columns in matrix\n",
    "        self.__columns_column = parameters['columns_column']\n",
    "        if 'values_column' in parameters:\n",
    "            self.__values_column = parameters['values_column']\n",
    "        else:\n",
    "            self.__values_column = None\n",
    "        self.__initialize_matrix()\n",
    "    \n",
    "    class UserNotFound(Exception):\n",
    "        \"\"\"Raise this execption when user is not found\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Cosine similarity calculated!\n",
      "SVD similarity calculated!\n",
      "Pearson similarity calculated!\n"
     ]
    }
   ],
   "source": [
    "user_assessment_similarity_measure = SimilarityMeasure({\n",
    "                                'dataframe' : user_assessment_scores,\n",
    "                                'index_column' : 'user_handle', 'columns_column' : 'assessment_tag',\n",
    "                                'values_column' : 'user_assessment_score'\n",
    "                                })\n",
    "user_assessment_similarity_measure.train_KNN_BaseLine()\n",
    "user_assessment_similarity_measure.calculate_cosine_similarity()\n",
    "user_assessment_similarity_measure.calculate_svd_similarity(dim_size = OPTIMIZED_DIM_SIZE_USER_ASSESSMENT)\n",
    "user_assessment_similarity_measure.calculate_pearson_similarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0\n",
      "9460  0.841721\n",
      "8412  0.789517\n",
      "5236  0.788519\n",
      "5694  0.766058\n",
      "1494  0.763504\n",
      "4048  0.729360\n",
      "840   0.709927\n",
      "9177  0.703223\n",
      "7997  0.696422\n",
      "             0\n",
      "8412  0.872228\n",
      "5694  0.790706\n",
      "5236  0.772198\n",
      "9460  0.766286\n",
      "840   0.760116\n",
      "9177  0.750983\n",
      "7246  0.747850\n",
      "1494  0.746683\n",
      "7997  0.745291\n",
      "        0\n",
      "988   1.0\n",
      "2233  1.0\n",
      "8074  1.0\n",
      "6395  1.0\n",
      "9467  0.5\n",
      "6060  0.5\n",
      "1095  0.5\n",
      "1097  0.5\n",
      "9872  0.5\n",
      "                    0\n",
      "user_handle          \n",
      "9460         1.000000\n",
      "5236         0.618466\n",
      "8412         0.614707\n",
      "5694         0.460403\n",
      "1494         0.443115\n",
      "4048         0.200535\n",
      "840          0.069679\n",
      "9177         0.021462\n",
      "7997         0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python27\\lib\\site-packages\\ipykernel_launcher.py:87: RuntimeWarning: invalid value encountered in divide\n"
     ]
    }
   ],
   "source": [
    "print(user_assessment_similarity_measure.get_cosine_similar_users('8887'))\n",
    "print(user_assessment_similarity_measure.get_svd_similar_users('8887'))\n",
    "print(user_assessment_similarity_measure.get_KNN_similar_users('8887'))\n",
    "print(user_assessment_similarity_measure.get_pearson_similar_users('8887'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity calculated!\n",
      "SVD similarity calculated!\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "user_interest_similarity_measure = SimilarityMeasure({\n",
    "                                'dataframe' : user_interests[['user_handle', 'interest_tag']].drop_duplicates(),\n",
    "                                'index_column' : 'user_handle', 'columns_column' : 'interest_tag'\n",
    "                                })\n",
    "user_interest_similarity_measure.calculate_cosine_similarity()\n",
    "user_interest_similarity_measure.calculate_svd_similarity(dim_size = OPTIMIZED_DIM_SIZE_USER_INTEREST)\n",
    "user_interest_similarity_measure.train_KNN_BaseLine()\n",
    "#user_interest_similarity_measure.calculate_pearson_similarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0\n",
      "4616  0.928571\n",
      "1090  0.925820\n",
      "8595  0.902007\n",
      "3576  0.897085\n",
      "2616  0.897085\n",
      "6701  0.894427\n",
      "7809  0.887244\n",
      "6581  0.887244\n",
      "3327  0.877328\n",
      "             0\n",
      "4616  0.758770\n",
      "4805  0.746552\n",
      "5622  0.744774\n",
      "7809  0.730792\n",
      "8595  0.698397\n",
      "4969  0.697123\n",
      "1090  0.694186\n",
      "2307  0.668801\n",
      "8112  0.668589\n",
      "        0\n",
      "6133  1.0\n",
      "6131  1.0\n",
      "6128  1.0\n",
      "6127  1.0\n",
      "6126  1.0\n",
      "6125  1.0\n",
      "6124  1.0\n",
      "6123  1.0\n",
      "6122  1.0\n",
      "Error - Pearson similarity not calculated!!!!\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(user_interest_similarity_measure.get_cosine_similar_users('8887'))\n",
    "print(user_interest_similarity_measure.get_svd_similar_users('8887'))\n",
    "print(user_interest_similarity_measure.get_KNN_similar_users('8887'))\n",
    "print(user_interest_similarity_measure.get_pearson_similar_users('8887'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD similarity calculated!\n",
      "Cosine similarity calculated!\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "user_courses_merge = pd.merge(left = user_course_views, right = course_tags, on = ['course_id'])\n",
    "grouped_users_course_tags = user_courses_merge[['user_handle', 'course_tags', 'view_time_seconds']]\\\n",
    "                                .groupby(['user_handle', 'course_tags'])\n",
    "meaned_group_user_courses_tag = grouped_users_course_tags.agg('mean').reset_index()\n",
    "user_courseview_tag_similarity_measure = SimilarityMeasure({\n",
    "                                'dataframe' : meaned_group_user_courses_tag[['user_handle', 'course_tags', \n",
    "                                                                             'view_time_seconds']],\n",
    "                                'index_column' : 'user_handle', 'columns_column' : 'course_tags',\n",
    "                                'values_column' : 'view_time_seconds'\n",
    "                                })\n",
    "user_courseview_tag_similarity_measure.calculate_svd_similarity(dim_size = OPTIMIZED_DIM_SIZE_USER_COURSE_TAG)\n",
    "user_courseview_tag_similarity_measure.calculate_cosine_similarity()\n",
    "user_courseview_tag_similarity_measure.train_KNN_BaseLine()\n",
    "#user_courseview_tag_similarity_measure.calculate_pearson_similarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0\n",
      "7817  0.436514\n",
      "5336  0.435969\n",
      "9275  0.433626\n",
      "3941  0.429487\n",
      "9735  0.416445\n",
      "238   0.415997\n",
      "4399  0.414570\n",
      "3301  0.412748\n",
      "4096  0.408434\n",
      "             0\n",
      "9376  0.503582\n",
      "2108  0.485718\n",
      "8824  0.477006\n",
      "238   0.468908\n",
      "9275  0.446999\n",
      "5336  0.440732\n",
      "7187  0.432308\n",
      "2791  0.429994\n",
      "6589  0.428914\n",
      "             0\n",
      "4832  0.999985\n",
      "611   0.999985\n",
      "1642  0.999983\n",
      "1240  0.999974\n",
      "7398  0.999970\n",
      "1865  0.999968\n",
      "2879  0.999933\n",
      "8142  0.999920\n",
      "4026  0.999891\n",
      "Error - Pearson similarity not calculated!!!!\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(user_courseview_tag_similarity_measure.get_cosine_similar_users('8887'))\n",
    "print(user_courseview_tag_similarity_measure.get_svd_similar_users('8887'))\n",
    "print(user_courseview_tag_similarity_measure.get_KNN_similar_users('8887'))\n",
    "print(user_courseview_tag_similarity_measure.get_pearson_similar_users('8887'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity calculated!\n",
      "SVD similarity calculated!\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "grouped_users_courses = user_course_views[['user_handle', 'course_id', 'view_time_seconds']]\\\n",
    "                                .groupby(['user_handle', 'course_id'])\n",
    "meaned_group_users_courses = grouped_users_courses.agg('mean').reset_index()\n",
    "user_courseview_similarity_measure = SimilarityMeasure({\n",
    "                                'dataframe' : meaned_group_users_courses[['user_handle', 'course_id', 'view_time_seconds']],\n",
    "                                'index_column' : 'user_handle', 'columns_column' : 'course_id',\n",
    "                                'values_column' : 'view_time_seconds'\n",
    "                                })\n",
    "user_courseview_similarity_measure.calculate_cosine_similarity()\n",
    "user_courseview_similarity_measure.calculate_svd_similarity(dim_size = OPTIMIZED_DIM_SIZE_USER_COURSE)\n",
    "user_courseview_similarity_measure.train_KNN_BaseLine()\n",
    "#user_courseview_similarity_measure.calculate_pearson_similarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0\n",
      "7077  0.313326\n",
      "9714  0.313326\n",
      "4550  0.313326\n",
      "9735  0.289132\n",
      "8423  0.283480\n",
      "7700  0.283480\n",
      "6815  0.278046\n",
      "5255  0.273364\n",
      "9938  0.256003\n",
      "             0\n",
      "6815  0.626078\n",
      "6265  0.617205\n",
      "3062  0.581514\n",
      "9241  0.578271\n",
      "5115  0.577414\n",
      "9714  0.570907\n",
      "7077  0.570907\n",
      "4550  0.570907\n",
      "5255  0.570770\n",
      "             0\n",
      "4972  0.999989\n",
      "2368  0.999985\n",
      "396   0.999975\n",
      "5234  0.999972\n",
      "6043  0.999969\n",
      "6293  0.999955\n",
      "7416  0.999852\n",
      "1891  0.999805\n",
      "7744  0.999783\n",
      "Error - Pearson similarity not calculated!!!!\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(user_courseview_similarity_measure.get_cosine_similar_users('8887'))\n",
    "print(user_courseview_similarity_measure.get_svd_similar_users('8887'))\n",
    "print(user_courseview_similarity_measure.get_KNN_similar_users('8887'))\n",
    "print(user_courseview_similarity_measure.get_pearson_similar_users('8887'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity calculated!\n",
      "SVD similarity calculated!\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "grouped_users_course_level = user_course_views[['user_handle', 'level', 'view_time_seconds']]\\\n",
    "                                .groupby(['user_handle', 'level'])\n",
    "meaned_group_users_course_level = grouped_users_course_level.agg('mean').reset_index()\n",
    "user_course_level_similarity_measure = SimilarityMeasure({\n",
    "                                'dataframe' : meaned_group_users_course_level[['user_handle', 'level', 'view_time_seconds']],\n",
    "                                'index_column' : 'user_handle', 'columns_column' : 'level',\n",
    "                                'values_column' : 'view_time_seconds'\n",
    "                                })\n",
    "user_course_level_similarity_measure.calculate_cosine_similarity()\n",
    "user_course_level_similarity_measure.calculate_svd_similarity(dim_size = OPTIMIZED_DIM_SIZE_USER_COURSE_LEVEL)\n",
    "user_course_level_similarity_measure.train_KNN_BaseLine()\n",
    "#user_course_level_similarity_measure.calculate_pearson_similarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0\n",
      "4312  1.0\n",
      "1463  1.0\n",
      "7028  1.0\n",
      "2713  1.0\n",
      "3157  1.0\n",
      "9014  1.0\n",
      "3942  1.0\n",
      "599   1.0\n",
      "5077  1.0\n",
      "             0\n",
      "9014  0.999996\n",
      "1664  0.999984\n",
      "3350  0.999982\n",
      "4656  0.999957\n",
      "1286  0.999920\n",
      "599   0.999916\n",
      "2798  0.999896\n",
      "1924  0.999829\n",
      "3968  0.999822\n",
      "             0\n",
      "5726  0.999999\n",
      "6297  0.999998\n",
      "1915  0.999997\n",
      "268   0.999995\n",
      "4086  0.999984\n",
      "8801  0.999983\n",
      "2333  0.999981\n",
      "2110  0.999971\n",
      "2424  0.999968\n",
      "Error - Pearson similarity not calculated!!!!\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(user_course_level_similarity_measure.get_cosine_similar_users('8887'))\n",
    "print(user_course_level_similarity_measure.get_svd_similar_users('8887'))\n",
    "print(user_course_level_similarity_measure.get_KNN_similar_users('8887'))\n",
    "print(user_course_level_similarity_measure.get_pearson_similar_users('8887'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error - Pearson similarity not calculated!!!!\n",
      "Error - Pearson similarity not calculated!!!!\n",
      "Error - Pearson similarity not calculated!!!!\n",
      "Error - Pearson similarity not calculated!!!!\n",
      "{'I': 1, 'CVT': 1, 'CV': 1, 'CL': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'A': 1, 'CL': 1, 'CV': 1, 'CVT': 1, 'I': 1}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimiliarUsers:\n",
    "    \"\"\"A warpper for the model that we will pickle and use for prediction\"\"\"\n",
    "    def get_merged_scores(self, cosine_similarity, svd_similarity, pearson_similarity, knn_similarity, score_type, weight):\n",
    "        svd_similarity = pd.DataFrame(svd_similarity).reset_index()\n",
    "        svd_similarity.columns = ['index', 'svd_' + score_type]\n",
    "        cosine_similarity = pd.DataFrame(cosine_similarity).reset_index()\n",
    "        cosine_similarity.columns = ['index', 'cosine_' + score_type]\n",
    "        merged = pd.merge(left = svd_similarity, right = cosine_similarity, on = 'index')\n",
    "        merged['total_'+score_type] = merged['svd_' + score_type] + merged['cosine_' + score_type]\n",
    "        n_scores = 2\n",
    "        if pearson_similarity is not None:\n",
    "            pearson_similarity = pd.DataFrame(pearson_similarity).reset_index()\n",
    "            pearson_similarity.columns = ['index', 'pearson_' + score_type]\n",
    "            merged = pd.merge(left = merged, right = pearson_similarity, on = 'index')\n",
    "            merged['total_'+score_type] = merged['total_'+score_type] + merged['pearson_' + score_type]\n",
    "            n_scores = n_scores + 1\n",
    "        merged['weighted_AVG_'+score_type] = (merged['total_'+score_type] * weight) / n_scores\n",
    "        return merged\n",
    "    \n",
    "    def get_assessment_similarity(self, user_handle):\n",
    "        if user_assessment_similarity_measure.isValidUser(user_handle):\n",
    "            cosine_similarity = self.user_assessment_similarity_measure.get_cosine_similar_users(user_handle, self.N_USERS_TO_COMPARE)\n",
    "            svd_similarity = self.user_assessment_similarity_measure.get_svd_similar_users(user_handle, self.N_USERS_TO_COMPARE)\n",
    "            pearson_similarity = self.user_assessment_similarity_measure.get_pearson_similar_users(user_handle, self.N_USERS_TO_COMPARE)\n",
    "            knn_similarity = self.user_assessment_similarity_measure.get_KNN_similar_users(user_handle, self.N_USERS_TO_COMPARE)\n",
    "            return self.get_merged_scores(cosine_similarity, svd_similarity, \n",
    "                                          pearson_similarity, knn_similarity, 'assessment', self.score_weights['A'])\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def get_interest_similarity(self, user_handle):\n",
    "        if user_interest_similarity_measure.isValidUser(user_handle):\n",
    "            cosine_similarity = self.user_interest_similarity_measure.get_cosine_similar_users(user_handle, self.N_USERS_TO_COMPARE)\n",
    "            svd_similarity = self.user_interest_similarity_measure.get_svd_similar_users(user_handle, self.N_USERS_TO_COMPARE)\n",
    "            pearson_similarity = self.user_interest_similarity_measure.get_pearson_similar_users(user_handle, self.N_USERS_TO_COMPARE)\n",
    "            knn_similarity = self.user_interest_similarity_measure.get_KNN_similar_users(user_handle, self.N_USERS_TO_COMPARE)\n",
    "            return self.get_merged_scores(cosine_similarity, svd_similarity, \n",
    "                                          pearson_similarity, knn_similarity, 'interest', self.score_weights['I'])\n",
    "        else :\n",
    "            return None\n",
    "        \n",
    "    def get_course_tag_similarity(self, user_handle): \n",
    "        if user_courseview_tag_similarity_measure.isValidUser(user_handle):\n",
    "            cosine_similarity = self.user_courseview_tag_similarity_measure.get_cosine_similar_users(user_handle, self.N_USERS_TO_COMPARE)\n",
    "            svd_similarity = self.user_courseview_tag_similarity_measure.get_svd_similar_users(user_handle, self.N_USERS_TO_COMPARE)\n",
    "            pearson_similarity = self.user_courseview_tag_similarity_measure.get_pearson_similar_users(user_handle, self.N_USERS_TO_COMPARE)\n",
    "            knn_similarity = self.user_courseview_tag_similarity_measure.get_KNN_similar_users(user_handle, self.N_USERS_TO_COMPARE)\n",
    "            return self.get_merged_scores(cosine_similarity, svd_similarity, \n",
    "                                          pearson_similarity, knn_similarity, 'course_tag', self.score_weights['CVT'])\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def get_course_view_similarity(self, user_handle): \n",
    "        if user_courseview_similarity_measure.isValidUser(user_handle):\n",
    "            cosine_similarity = self.user_courseview_similarity_measure.get_cosine_similar_users(user_handle, self.N_USERS_TO_COMPARE)\n",
    "            svd_similarity = self.user_courseview_similarity_measure.get_svd_similar_users(user_handle, self.N_USERS_TO_COMPARE)\n",
    "            pearson_similarity = self.user_courseview_similarity_measure.get_pearson_similar_users(user_handle, self.N_USERS_TO_COMPARE)\n",
    "            knn_similarity = self.user_courseview_similarity_measure.get_KNN_similar_users(user_handle, self.N_USERS_TO_COMPARE)\n",
    "            return self.get_merged_scores(cosine_similarity, svd_similarity, \n",
    "                                          pearson_similarity, knn_similarity, 'course_view', self.score_weights['CV'])\n",
    "        else:\n",
    "            return None\n",
    "    def get_course_level_similarity(self, user_handle):\n",
    "        if user_course_level_similarity_measure.isValidUser(user_handle):\n",
    "            cosine_similarity = self.user_course_level_similarity_measure.get_cosine_similar_users(user_handle, self.N_USERS_TO_COMPARE)\n",
    "            svd_similarity = self.user_course_level_similarity_measure.get_svd_similar_users(user_handle, self.N_USERS_TO_COMPARE)\n",
    "            pearson_similarity = self.user_course_level_similarity_measure.get_pearson_similar_users(user_handle, self.N_USERS_TO_COMPARE)\n",
    "            knn_similarity = self.user_course_level_similarity_measure.get_KNN_similar_users(user_handle, self.N_USERS_TO_COMPARE)\n",
    "            return self.get_merged_scores(cosine_similarity, svd_similarity, \n",
    "                                          pearson_similarity, knn_similarity, 'course_level', self.score_weights['CL'])\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def calculate_total_score(self, merged_similarity):\n",
    "        total_columns = [s for s in merged_similarity.columns if 'AVG' in s]\n",
    "        print(self.score_weights)\n",
    "        merged_similarity['AVG_OF_ALL'] = merged_similarity[total_columns].sum(axis=1) / sum(self.score_weights.values())\n",
    "        merged_similarity = merged_similarity.sort_values(by=['AVG_OF_ALL'], ascending = False)\n",
    "        return merged_similarity\n",
    "    \n",
    "    def set_score_weights(self, new_weights):\n",
    "        if new_weights is not None:\n",
    "            self.score_weights = new_weights\n",
    "        else:\n",
    "            print(\"Invalid weights passed!!!!\")\n",
    "            \n",
    "    def get_similar_users(self, user_handle):\n",
    "        #store old weight because the weights dictionary will change if any score not present.\n",
    "        temp_weights = self.score_weights.copy()\n",
    "        merged_similarity = None\n",
    "        \n",
    "        user_assessment_similarity = self.get_assessment_similarity(user_handle)\n",
    "        user_interest_similarity = self.get_interest_similarity(user_handle)\n",
    "        \n",
    "        if (user_assessment_similarity is not None) and (user_interest_similarity is not None):\n",
    "            merged_similarity = pd.merge(user_assessment_similarity, user_interest_similarity, on = 'index')\n",
    "        elif user_assessment_similarity is not None:\n",
    "            del self.score_weights['I']\n",
    "            merged_similarity = user_assessment_similarity\n",
    "        elif user_interest_similarity is not None:\n",
    "            del self.score_weights['A']\n",
    "            merged_similarity = user_interest_similarity\n",
    "            \n",
    "        user_coursetag_similarity = self.get_course_tag_similarity(user_handle)\n",
    "        if (user_coursetag_similarity is not None):\n",
    "            merged_similarity = pd.merge(merged_similarity, user_coursetag_similarity, on = 'index')\n",
    "        else:\n",
    "            del self.score_weights['CVT']\n",
    "            \n",
    "        user_courseview_similarity = self.get_course_view_similarity(user_handle)\n",
    "        if (user_courseview_similarity is not None):\n",
    "            merged_similarity = pd.merge(merged_similarity, user_courseview_similarity, on = 'index')\n",
    "        else:\n",
    "            del self.score_weights['CV']\n",
    "            \n",
    "        user_courselevel_similarity = self.get_course_level_similarity(user_handle)\n",
    "        if (user_courselevel_similarity is not None):\n",
    "            merged_similarity = pd.merge(merged_similarity, user_courselevel_similarity, on = 'index')\n",
    "        else:\n",
    "            del self.score_weights['CL']\n",
    "            \n",
    "        if merged_similarity is not None:\n",
    "            merged_similarity = self.calculate_total_score(merged_similarity)\n",
    "            merged_similarity = merged_similarity.rename(index=str, columns={\"index\": \"user_handle\"})\n",
    "        \n",
    "        # set the weight back to original\n",
    "        self.score_weights = temp_weights\n",
    "        return merged_similarity\n",
    "    \n",
    "    def __init__(self, parameters):\n",
    "        self.user_assessment_similarity_measure = parameters['user_assessment_similarity_measure']\n",
    "        self.user_interest_similarity_measure = parameters['user_interest_similarity_measure']\n",
    "        self.user_courseview_tag_similarity_measure = parameters['user_courseview_tag_similarity_measure']\n",
    "        self.user_courseview_similarity_measure = parameters['user_courseview_similarity_measure']\n",
    "        self.user_course_level_similarity_measure = parameters['user_course_level_similarity_measure']\n",
    "        if 'score_weights' in parameters:\n",
    "            self.score_weights = parameters['score_weights']\n",
    "        else:\n",
    "            # By default give equal weight to all the scores\n",
    "            # A = Assessment, I = User Interest, CVT = Course view tags, CV = Course View, CL = Course Level\n",
    "            self.score_weights = {'A' : 1, 'I' : 1, 'CVT' : 1, 'CV' : 1, 'CL' : 1}\n",
    "        self.N_USERS_TO_COMPARE = 10000\n",
    "        \n",
    "similarity_measures = {\n",
    "    'user_assessment_similarity_measure' : user_assessment_similarity_measure,\n",
    "    'user_interest_similarity_measure' : user_interest_similarity_measure,\n",
    "    'user_courseview_tag_similarity_measure' : user_courseview_tag_similarity_measure,\n",
    "    'user_courseview_similarity_measure' : user_courseview_similarity_measure,\n",
    "    'user_course_level_similarity_measure' : user_course_level_similarity_measure\n",
    "}\n",
    "similar_users_model = SimiliarUsers(similarity_measures)\n",
    "similar_users_df = similar_users_model.get_similar_users('1')\n",
    "similar_users_model.score_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error - Pearson similarity not calculated!!!!\n",
      "Error - Pearson similarity not calculated!!!!\n",
      "Error - Pearson similarity not calculated!!!!\n",
      "Error - Pearson similarity not calculated!!!!\n",
      "{'I': 4, 'CVT': 1, 'CV': 2, 'CL': 1}\n",
      "Index([u'user_handle', u'svd_interest', u'cosine_interest', u'total_interest',\n",
      "       u'weighted_AVG_interest', u'svd_course_tag', u'cosine_course_tag',\n",
      "       u'total_course_tag', u'weighted_AVG_course_tag', u'svd_course_view',\n",
      "       u'cosine_course_view', u'total_course_view',\n",
      "       u'weighted_AVG_course_view', u'svd_course_level',\n",
      "       u'cosine_course_level', u'total_course_level',\n",
      "       u'weighted_AVG_course_level', u'AVG_OF_ALL'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_handle</th>\n",
       "      <th>svd_interest</th>\n",
       "      <th>cosine_interest</th>\n",
       "      <th>total_interest</th>\n",
       "      <th>weighted_AVG_interest</th>\n",
       "      <th>svd_course_tag</th>\n",
       "      <th>cosine_course_tag</th>\n",
       "      <th>total_course_tag</th>\n",
       "      <th>weighted_AVG_course_tag</th>\n",
       "      <th>svd_course_view</th>\n",
       "      <th>cosine_course_view</th>\n",
       "      <th>total_course_view</th>\n",
       "      <th>weighted_AVG_course_view</th>\n",
       "      <th>svd_course_level</th>\n",
       "      <th>cosine_course_level</th>\n",
       "      <th>total_course_level</th>\n",
       "      <th>weighted_AVG_course_level</th>\n",
       "      <th>AVG_OF_ALL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>691</td>\n",
       "      <td>0.350182</td>\n",
       "      <td>0.684712</td>\n",
       "      <td>1.034895</td>\n",
       "      <td>2.069789</td>\n",
       "      <td>0.774473</td>\n",
       "      <td>0.714715</td>\n",
       "      <td>1.489188</td>\n",
       "      <td>0.744594</td>\n",
       "      <td>0.661832</td>\n",
       "      <td>0.505936</td>\n",
       "      <td>1.167768</td>\n",
       "      <td>1.167768</td>\n",
       "      <td>0.127815</td>\n",
       "      <td>0.727447</td>\n",
       "      <td>0.855262</td>\n",
       "      <td>0.427631</td>\n",
       "      <td>0.551223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>6251</td>\n",
       "      <td>0.281661</td>\n",
       "      <td>0.699234</td>\n",
       "      <td>0.980895</td>\n",
       "      <td>1.961790</td>\n",
       "      <td>0.774473</td>\n",
       "      <td>0.714715</td>\n",
       "      <td>1.489188</td>\n",
       "      <td>0.744594</td>\n",
       "      <td>0.661832</td>\n",
       "      <td>0.505936</td>\n",
       "      <td>1.167768</td>\n",
       "      <td>1.167768</td>\n",
       "      <td>0.127815</td>\n",
       "      <td>0.727447</td>\n",
       "      <td>0.855262</td>\n",
       "      <td>0.427631</td>\n",
       "      <td>0.537723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3200</td>\n",
       "      <td>0.725886</td>\n",
       "      <td>0.855162</td>\n",
       "      <td>1.581048</td>\n",
       "      <td>3.162097</td>\n",
       "      <td>-0.052290</td>\n",
       "      <td>0.148025</td>\n",
       "      <td>0.095735</td>\n",
       "      <td>0.047867</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>0.058832</td>\n",
       "      <td>0.062364</td>\n",
       "      <td>0.062364</td>\n",
       "      <td>0.832783</td>\n",
       "      <td>0.989153</td>\n",
       "      <td>1.821936</td>\n",
       "      <td>0.910968</td>\n",
       "      <td>0.522912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>8146</td>\n",
       "      <td>0.306555</td>\n",
       "      <td>0.708142</td>\n",
       "      <td>1.014697</td>\n",
       "      <td>2.029393</td>\n",
       "      <td>0.650727</td>\n",
       "      <td>0.650709</td>\n",
       "      <td>1.301435</td>\n",
       "      <td>0.650718</td>\n",
       "      <td>0.481604</td>\n",
       "      <td>0.369292</td>\n",
       "      <td>0.850896</td>\n",
       "      <td>0.850896</td>\n",
       "      <td>0.290758</td>\n",
       "      <td>0.979237</td>\n",
       "      <td>1.269994</td>\n",
       "      <td>0.634997</td>\n",
       "      <td>0.520750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9561</td>\n",
       "      <td>0.524617</td>\n",
       "      <td>0.816094</td>\n",
       "      <td>1.340712</td>\n",
       "      <td>2.681423</td>\n",
       "      <td>0.269377</td>\n",
       "      <td>0.245916</td>\n",
       "      <td>0.515292</td>\n",
       "      <td>0.257646</td>\n",
       "      <td>0.171234</td>\n",
       "      <td>0.143115</td>\n",
       "      <td>0.314350</td>\n",
       "      <td>0.314350</td>\n",
       "      <td>0.651999</td>\n",
       "      <td>0.976047</td>\n",
       "      <td>1.628045</td>\n",
       "      <td>0.814023</td>\n",
       "      <td>0.508430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>680</td>\n",
       "      <td>0.658800</td>\n",
       "      <td>0.886405</td>\n",
       "      <td>1.545205</td>\n",
       "      <td>3.090410</td>\n",
       "      <td>-0.063299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.063299</td>\n",
       "      <td>-0.031650</td>\n",
       "      <td>-0.018236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.018236</td>\n",
       "      <td>-0.018236</td>\n",
       "      <td>0.958799</td>\n",
       "      <td>0.950850</td>\n",
       "      <td>1.909649</td>\n",
       "      <td>0.954825</td>\n",
       "      <td>0.499419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3557</td>\n",
       "      <td>0.858217</td>\n",
       "      <td>0.915584</td>\n",
       "      <td>1.773800</td>\n",
       "      <td>3.547601</td>\n",
       "      <td>-0.168324</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.168324</td>\n",
       "      <td>-0.084162</td>\n",
       "      <td>0.011961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011961</td>\n",
       "      <td>0.011961</td>\n",
       "      <td>0.158436</td>\n",
       "      <td>0.727447</td>\n",
       "      <td>0.885883</td>\n",
       "      <td>0.442942</td>\n",
       "      <td>0.489793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2835</td>\n",
       "      <td>0.399411</td>\n",
       "      <td>0.690654</td>\n",
       "      <td>1.090065</td>\n",
       "      <td>2.180131</td>\n",
       "      <td>0.344764</td>\n",
       "      <td>0.318295</td>\n",
       "      <td>0.663060</td>\n",
       "      <td>0.331530</td>\n",
       "      <td>0.296326</td>\n",
       "      <td>0.179813</td>\n",
       "      <td>0.476139</td>\n",
       "      <td>0.476139</td>\n",
       "      <td>0.459806</td>\n",
       "      <td>0.990342</td>\n",
       "      <td>1.450148</td>\n",
       "      <td>0.725074</td>\n",
       "      <td>0.464109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6148</td>\n",
       "      <td>0.602835</td>\n",
       "      <td>0.793588</td>\n",
       "      <td>1.396423</td>\n",
       "      <td>2.792846</td>\n",
       "      <td>0.033653</td>\n",
       "      <td>0.199804</td>\n",
       "      <td>0.233457</td>\n",
       "      <td>0.116728</td>\n",
       "      <td>0.222695</td>\n",
       "      <td>0.096576</td>\n",
       "      <td>0.319271</td>\n",
       "      <td>0.319271</td>\n",
       "      <td>-0.011053</td>\n",
       "      <td>0.965860</td>\n",
       "      <td>0.954808</td>\n",
       "      <td>0.477404</td>\n",
       "      <td>0.463281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2863</th>\n",
       "      <td>6147</td>\n",
       "      <td>0.060751</td>\n",
       "      <td>0.591608</td>\n",
       "      <td>0.652358</td>\n",
       "      <td>1.304717</td>\n",
       "      <td>0.774473</td>\n",
       "      <td>0.714715</td>\n",
       "      <td>1.489188</td>\n",
       "      <td>0.744594</td>\n",
       "      <td>0.661832</td>\n",
       "      <td>0.505936</td>\n",
       "      <td>1.167768</td>\n",
       "      <td>1.167768</td>\n",
       "      <td>0.205153</td>\n",
       "      <td>0.727447</td>\n",
       "      <td>0.932600</td>\n",
       "      <td>0.466300</td>\n",
       "      <td>0.460422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1968</td>\n",
       "      <td>0.841464</td>\n",
       "      <td>0.873326</td>\n",
       "      <td>1.714790</td>\n",
       "      <td>3.429579</td>\n",
       "      <td>-0.045452</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.045452</td>\n",
       "      <td>-0.022726</td>\n",
       "      <td>-0.031499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.031499</td>\n",
       "      <td>-0.031499</td>\n",
       "      <td>0.042376</td>\n",
       "      <td>0.570356</td>\n",
       "      <td>0.612732</td>\n",
       "      <td>0.306366</td>\n",
       "      <td>0.460215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4391</td>\n",
       "      <td>0.763605</td>\n",
       "      <td>0.910259</td>\n",
       "      <td>1.673864</td>\n",
       "      <td>3.347728</td>\n",
       "      <td>0.059635</td>\n",
       "      <td>0.191530</td>\n",
       "      <td>0.251166</td>\n",
       "      <td>0.125583</td>\n",
       "      <td>0.056307</td>\n",
       "      <td>0.057330</td>\n",
       "      <td>0.113637</td>\n",
       "      <td>0.113637</td>\n",
       "      <td>-0.807918</td>\n",
       "      <td>0.929234</td>\n",
       "      <td>0.121316</td>\n",
       "      <td>0.060658</td>\n",
       "      <td>0.455951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>59</td>\n",
       "      <td>0.240150</td>\n",
       "      <td>0.445367</td>\n",
       "      <td>0.685517</td>\n",
       "      <td>1.371035</td>\n",
       "      <td>0.774473</td>\n",
       "      <td>0.714715</td>\n",
       "      <td>1.489188</td>\n",
       "      <td>0.744594</td>\n",
       "      <td>0.661832</td>\n",
       "      <td>0.505936</td>\n",
       "      <td>1.167768</td>\n",
       "      <td>1.167768</td>\n",
       "      <td>-0.004212</td>\n",
       "      <td>0.727447</td>\n",
       "      <td>0.723235</td>\n",
       "      <td>0.361617</td>\n",
       "      <td>0.455627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>7045</td>\n",
       "      <td>0.138695</td>\n",
       "      <td>0.492805</td>\n",
       "      <td>0.631501</td>\n",
       "      <td>1.263001</td>\n",
       "      <td>0.774473</td>\n",
       "      <td>0.714715</td>\n",
       "      <td>1.489188</td>\n",
       "      <td>0.744594</td>\n",
       "      <td>0.661832</td>\n",
       "      <td>0.505936</td>\n",
       "      <td>1.167768</td>\n",
       "      <td>1.167768</td>\n",
       "      <td>0.127815</td>\n",
       "      <td>0.727447</td>\n",
       "      <td>0.855262</td>\n",
       "      <td>0.427631</td>\n",
       "      <td>0.450374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1712</td>\n",
       "      <td>0.691798</td>\n",
       "      <td>0.886547</td>\n",
       "      <td>1.578345</td>\n",
       "      <td>3.156690</td>\n",
       "      <td>-0.066466</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.066466</td>\n",
       "      <td>-0.033233</td>\n",
       "      <td>0.005688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005688</td>\n",
       "      <td>0.005688</td>\n",
       "      <td>0.190477</td>\n",
       "      <td>0.727447</td>\n",
       "      <td>0.917925</td>\n",
       "      <td>0.458962</td>\n",
       "      <td>0.448513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>3310</td>\n",
       "      <td>0.401217</td>\n",
       "      <td>0.792825</td>\n",
       "      <td>1.194042</td>\n",
       "      <td>2.388083</td>\n",
       "      <td>0.420977</td>\n",
       "      <td>0.341064</td>\n",
       "      <td>0.762041</td>\n",
       "      <td>0.381020</td>\n",
       "      <td>0.225043</td>\n",
       "      <td>0.180720</td>\n",
       "      <td>0.405763</td>\n",
       "      <td>0.405763</td>\n",
       "      <td>-0.154091</td>\n",
       "      <td>0.918260</td>\n",
       "      <td>0.764169</td>\n",
       "      <td>0.382084</td>\n",
       "      <td>0.444619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>9665</td>\n",
       "      <td>0.351045</td>\n",
       "      <td>0.355117</td>\n",
       "      <td>0.706162</td>\n",
       "      <td>1.412325</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.702056</td>\n",
       "      <td>1.463961</td>\n",
       "      <td>0.731980</td>\n",
       "      <td>0.615043</td>\n",
       "      <td>0.255716</td>\n",
       "      <td>0.870759</td>\n",
       "      <td>0.870759</td>\n",
       "      <td>0.196332</td>\n",
       "      <td>0.859816</td>\n",
       "      <td>1.056148</td>\n",
       "      <td>0.528074</td>\n",
       "      <td>0.442892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6531</td>\n",
       "      <td>0.732368</td>\n",
       "      <td>0.902378</td>\n",
       "      <td>1.634746</td>\n",
       "      <td>3.269491</td>\n",
       "      <td>-0.016901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.016901</td>\n",
       "      <td>-0.008450</td>\n",
       "      <td>-0.029037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.029037</td>\n",
       "      <td>-0.029037</td>\n",
       "      <td>-0.121468</td>\n",
       "      <td>0.727447</td>\n",
       "      <td>0.605979</td>\n",
       "      <td>0.302990</td>\n",
       "      <td>0.441874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>8260</td>\n",
       "      <td>0.239238</td>\n",
       "      <td>0.591301</td>\n",
       "      <td>0.830539</td>\n",
       "      <td>1.661078</td>\n",
       "      <td>0.485091</td>\n",
       "      <td>0.412373</td>\n",
       "      <td>0.897464</td>\n",
       "      <td>0.448732</td>\n",
       "      <td>0.567814</td>\n",
       "      <td>0.337980</td>\n",
       "      <td>0.905794</td>\n",
       "      <td>0.905794</td>\n",
       "      <td>0.046818</td>\n",
       "      <td>0.923191</td>\n",
       "      <td>0.970008</td>\n",
       "      <td>0.485004</td>\n",
       "      <td>0.437576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>4316</td>\n",
       "      <td>0.280308</td>\n",
       "      <td>0.298347</td>\n",
       "      <td>0.578655</td>\n",
       "      <td>1.157311</td>\n",
       "      <td>0.774473</td>\n",
       "      <td>0.714715</td>\n",
       "      <td>1.489188</td>\n",
       "      <td>0.744594</td>\n",
       "      <td>0.661832</td>\n",
       "      <td>0.505936</td>\n",
       "      <td>1.167768</td>\n",
       "      <td>1.167768</td>\n",
       "      <td>0.127815</td>\n",
       "      <td>0.727447</td>\n",
       "      <td>0.855262</td>\n",
       "      <td>0.427631</td>\n",
       "      <td>0.437163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>5576</td>\n",
       "      <td>0.429238</td>\n",
       "      <td>0.765320</td>\n",
       "      <td>1.194558</td>\n",
       "      <td>2.389116</td>\n",
       "      <td>0.200894</td>\n",
       "      <td>0.298186</td>\n",
       "      <td>0.499080</td>\n",
       "      <td>0.249540</td>\n",
       "      <td>0.205854</td>\n",
       "      <td>0.134537</td>\n",
       "      <td>0.340391</td>\n",
       "      <td>0.340391</td>\n",
       "      <td>0.004174</td>\n",
       "      <td>0.920608</td>\n",
       "      <td>0.924782</td>\n",
       "      <td>0.462391</td>\n",
       "      <td>0.430180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>8472</td>\n",
       "      <td>0.255593</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>0.771990</td>\n",
       "      <td>1.543981</td>\n",
       "      <td>0.635459</td>\n",
       "      <td>0.609879</td>\n",
       "      <td>1.245338</td>\n",
       "      <td>0.622669</td>\n",
       "      <td>0.559034</td>\n",
       "      <td>0.382814</td>\n",
       "      <td>0.941848</td>\n",
       "      <td>0.941848</td>\n",
       "      <td>-0.290231</td>\n",
       "      <td>0.923333</td>\n",
       "      <td>0.633102</td>\n",
       "      <td>0.316551</td>\n",
       "      <td>0.428131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>983</td>\n",
       "      <td>0.462450</td>\n",
       "      <td>0.665611</td>\n",
       "      <td>1.128061</td>\n",
       "      <td>2.256123</td>\n",
       "      <td>0.440043</td>\n",
       "      <td>0.313650</td>\n",
       "      <td>0.753693</td>\n",
       "      <td>0.376846</td>\n",
       "      <td>0.247503</td>\n",
       "      <td>0.175440</td>\n",
       "      <td>0.422943</td>\n",
       "      <td>0.422943</td>\n",
       "      <td>-0.182685</td>\n",
       "      <td>0.916216</td>\n",
       "      <td>0.733531</td>\n",
       "      <td>0.366765</td>\n",
       "      <td>0.427835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>9530</td>\n",
       "      <td>0.200952</td>\n",
       "      <td>0.668153</td>\n",
       "      <td>0.869105</td>\n",
       "      <td>1.738210</td>\n",
       "      <td>0.469814</td>\n",
       "      <td>0.361741</td>\n",
       "      <td>0.831556</td>\n",
       "      <td>0.415778</td>\n",
       "      <td>0.401836</td>\n",
       "      <td>0.257348</td>\n",
       "      <td>0.659184</td>\n",
       "      <td>0.659184</td>\n",
       "      <td>0.261607</td>\n",
       "      <td>0.913943</td>\n",
       "      <td>1.175550</td>\n",
       "      <td>0.587775</td>\n",
       "      <td>0.425118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8678</td>\n",
       "      <td>0.521813</td>\n",
       "      <td>0.620642</td>\n",
       "      <td>1.142456</td>\n",
       "      <td>2.284911</td>\n",
       "      <td>0.150282</td>\n",
       "      <td>0.124052</td>\n",
       "      <td>0.274334</td>\n",
       "      <td>0.137167</td>\n",
       "      <td>0.310178</td>\n",
       "      <td>0.110196</td>\n",
       "      <td>0.420374</td>\n",
       "      <td>0.420374</td>\n",
       "      <td>0.164827</td>\n",
       "      <td>0.924237</td>\n",
       "      <td>1.089064</td>\n",
       "      <td>0.544532</td>\n",
       "      <td>0.423373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3603</td>\n",
       "      <td>0.499646</td>\n",
       "      <td>0.708142</td>\n",
       "      <td>1.207788</td>\n",
       "      <td>2.415576</td>\n",
       "      <td>0.157076</td>\n",
       "      <td>0.197628</td>\n",
       "      <td>0.354704</td>\n",
       "      <td>0.177352</td>\n",
       "      <td>0.086966</td>\n",
       "      <td>0.088251</td>\n",
       "      <td>0.175217</td>\n",
       "      <td>0.175217</td>\n",
       "      <td>0.295123</td>\n",
       "      <td>0.920526</td>\n",
       "      <td>1.215650</td>\n",
       "      <td>0.607825</td>\n",
       "      <td>0.421996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>5540</td>\n",
       "      <td>0.435326</td>\n",
       "      <td>0.762443</td>\n",
       "      <td>1.197769</td>\n",
       "      <td>2.395537</td>\n",
       "      <td>0.211454</td>\n",
       "      <td>0.260562</td>\n",
       "      <td>0.472017</td>\n",
       "      <td>0.236008</td>\n",
       "      <td>0.089816</td>\n",
       "      <td>0.156071</td>\n",
       "      <td>0.245887</td>\n",
       "      <td>0.245887</td>\n",
       "      <td>0.058427</td>\n",
       "      <td>0.924041</td>\n",
       "      <td>0.982467</td>\n",
       "      <td>0.491234</td>\n",
       "      <td>0.421083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>4375</td>\n",
       "      <td>0.220919</td>\n",
       "      <td>0.478091</td>\n",
       "      <td>0.699010</td>\n",
       "      <td>1.398021</td>\n",
       "      <td>0.518283</td>\n",
       "      <td>0.409214</td>\n",
       "      <td>0.927497</td>\n",
       "      <td>0.463748</td>\n",
       "      <td>0.606484</td>\n",
       "      <td>0.289676</td>\n",
       "      <td>0.896160</td>\n",
       "      <td>0.896160</td>\n",
       "      <td>0.282763</td>\n",
       "      <td>0.913599</td>\n",
       "      <td>1.196362</td>\n",
       "      <td>0.598181</td>\n",
       "      <td>0.419514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>7537</td>\n",
       "      <td>0.191582</td>\n",
       "      <td>0.478091</td>\n",
       "      <td>0.669673</td>\n",
       "      <td>1.339347</td>\n",
       "      <td>0.681331</td>\n",
       "      <td>0.585638</td>\n",
       "      <td>1.266969</td>\n",
       "      <td>0.633484</td>\n",
       "      <td>0.603174</td>\n",
       "      <td>0.439321</td>\n",
       "      <td>1.042495</td>\n",
       "      <td>1.042495</td>\n",
       "      <td>-0.060262</td>\n",
       "      <td>0.727447</td>\n",
       "      <td>0.667185</td>\n",
       "      <td>0.333593</td>\n",
       "      <td>0.418615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8567</td>\n",
       "      <td>0.675377</td>\n",
       "      <td>0.768424</td>\n",
       "      <td>1.443802</td>\n",
       "      <td>2.887604</td>\n",
       "      <td>-0.074735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.074735</td>\n",
       "      <td>-0.037367</td>\n",
       "      <td>0.002929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002929</td>\n",
       "      <td>0.002929</td>\n",
       "      <td>0.066851</td>\n",
       "      <td>0.921535</td>\n",
       "      <td>0.988386</td>\n",
       "      <td>0.494193</td>\n",
       "      <td>0.418420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6456</th>\n",
       "      <td>1993</td>\n",
       "      <td>-0.075590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.075590</td>\n",
       "      <td>-0.151180</td>\n",
       "      <td>-0.048631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.048631</td>\n",
       "      <td>-0.024316</td>\n",
       "      <td>-0.032030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.032030</td>\n",
       "      <td>-0.032030</td>\n",
       "      <td>-0.988856</td>\n",
       "      <td>0.959473</td>\n",
       "      <td>-0.029383</td>\n",
       "      <td>-0.014691</td>\n",
       "      <td>-0.027777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8691</th>\n",
       "      <td>7739</td>\n",
       "      <td>-0.240927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.240927</td>\n",
       "      <td>-0.481855</td>\n",
       "      <td>-0.088060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.088060</td>\n",
       "      <td>-0.044030</td>\n",
       "      <td>0.006801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006801</td>\n",
       "      <td>0.006801</td>\n",
       "      <td>-0.134208</td>\n",
       "      <td>0.727447</td>\n",
       "      <td>0.593240</td>\n",
       "      <td>0.296620</td>\n",
       "      <td>-0.027808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8307</th>\n",
       "      <td>9362</td>\n",
       "      <td>-0.181539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.181539</td>\n",
       "      <td>-0.363079</td>\n",
       "      <td>-0.079871</td>\n",
       "      <td>0.078956</td>\n",
       "      <td>-0.000914</td>\n",
       "      <td>-0.000457</td>\n",
       "      <td>-0.050199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.050199</td>\n",
       "      <td>-0.050199</td>\n",
       "      <td>-0.604570</td>\n",
       "      <td>0.983832</td>\n",
       "      <td>0.379262</td>\n",
       "      <td>0.189631</td>\n",
       "      <td>-0.028013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7091</th>\n",
       "      <td>9024</td>\n",
       "      <td>-0.103350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.103350</td>\n",
       "      <td>-0.206700</td>\n",
       "      <td>-0.121784</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.121784</td>\n",
       "      <td>-0.060892</td>\n",
       "      <td>0.013180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013180</td>\n",
       "      <td>0.013180</td>\n",
       "      <td>-0.925436</td>\n",
       "      <td>0.981083</td>\n",
       "      <td>0.055647</td>\n",
       "      <td>0.027823</td>\n",
       "      <td>-0.028324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8533</th>\n",
       "      <td>8480</td>\n",
       "      <td>-0.206605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.206605</td>\n",
       "      <td>-0.413209</td>\n",
       "      <td>-0.204639</td>\n",
       "      <td>0.029966</td>\n",
       "      <td>-0.174673</td>\n",
       "      <td>-0.087337</td>\n",
       "      <td>-0.117066</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.117066</td>\n",
       "      <td>-0.117066</td>\n",
       "      <td>-0.157948</td>\n",
       "      <td>0.922817</td>\n",
       "      <td>0.764869</td>\n",
       "      <td>0.382435</td>\n",
       "      <td>-0.029397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7792</th>\n",
       "      <td>9127</td>\n",
       "      <td>-0.145494</td>\n",
       "      <td>0.106904</td>\n",
       "      <td>-0.038589</td>\n",
       "      <td>-0.077178</td>\n",
       "      <td>-0.120377</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.120377</td>\n",
       "      <td>-0.060189</td>\n",
       "      <td>-0.090801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.090801</td>\n",
       "      <td>-0.090801</td>\n",
       "      <td>-0.968116</td>\n",
       "      <td>0.952961</td>\n",
       "      <td>-0.015155</td>\n",
       "      <td>-0.007577</td>\n",
       "      <td>-0.029468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7465</th>\n",
       "      <td>7143</td>\n",
       "      <td>-0.125601</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.125601</td>\n",
       "      <td>-0.251201</td>\n",
       "      <td>-0.029324</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.029324</td>\n",
       "      <td>-0.014662</td>\n",
       "      <td>0.013120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013120</td>\n",
       "      <td>0.013120</td>\n",
       "      <td>-0.928441</td>\n",
       "      <td>0.960413</td>\n",
       "      <td>0.031972</td>\n",
       "      <td>0.015986</td>\n",
       "      <td>-0.029595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8285</th>\n",
       "      <td>7951</td>\n",
       "      <td>-0.179730</td>\n",
       "      <td>0.042258</td>\n",
       "      <td>-0.137472</td>\n",
       "      <td>-0.274944</td>\n",
       "      <td>0.019186</td>\n",
       "      <td>0.096499</td>\n",
       "      <td>0.115685</td>\n",
       "      <td>0.057843</td>\n",
       "      <td>-0.091558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.091558</td>\n",
       "      <td>-0.091558</td>\n",
       "      <td>-0.835416</td>\n",
       "      <td>0.978365</td>\n",
       "      <td>0.142948</td>\n",
       "      <td>0.071474</td>\n",
       "      <td>-0.029648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8409</th>\n",
       "      <td>3247</td>\n",
       "      <td>-0.189661</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.189661</td>\n",
       "      <td>-0.379323</td>\n",
       "      <td>-0.069126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.069126</td>\n",
       "      <td>-0.034563</td>\n",
       "      <td>-0.074356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.074356</td>\n",
       "      <td>-0.074356</td>\n",
       "      <td>-0.229392</td>\n",
       "      <td>0.727447</td>\n",
       "      <td>0.498055</td>\n",
       "      <td>0.249027</td>\n",
       "      <td>-0.029902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7209</th>\n",
       "      <td>9812</td>\n",
       "      <td>-0.110603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.110603</td>\n",
       "      <td>-0.221206</td>\n",
       "      <td>-0.009344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.009344</td>\n",
       "      <td>-0.004672</td>\n",
       "      <td>-0.010603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.010603</td>\n",
       "      <td>-0.010603</td>\n",
       "      <td>-0.998010</td>\n",
       "      <td>0.971940</td>\n",
       "      <td>-0.026071</td>\n",
       "      <td>-0.013035</td>\n",
       "      <td>-0.031190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8621</th>\n",
       "      <td>2203</td>\n",
       "      <td>-0.221767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.221767</td>\n",
       "      <td>-0.443534</td>\n",
       "      <td>-0.033929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.033929</td>\n",
       "      <td>-0.016964</td>\n",
       "      <td>-0.044119</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.044119</td>\n",
       "      <td>-0.044119</td>\n",
       "      <td>-0.494157</td>\n",
       "      <td>0.997890</td>\n",
       "      <td>0.503733</td>\n",
       "      <td>0.251867</td>\n",
       "      <td>-0.031594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7636</th>\n",
       "      <td>5610</td>\n",
       "      <td>-0.135448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.135448</td>\n",
       "      <td>-0.270896</td>\n",
       "      <td>-0.011489</td>\n",
       "      <td>0.047399</td>\n",
       "      <td>0.035910</td>\n",
       "      <td>0.017955</td>\n",
       "      <td>-0.023226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.023226</td>\n",
       "      <td>-0.023226</td>\n",
       "      <td>-0.975229</td>\n",
       "      <td>0.968037</td>\n",
       "      <td>-0.007192</td>\n",
       "      <td>-0.003596</td>\n",
       "      <td>-0.034970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8484</th>\n",
       "      <td>6337</td>\n",
       "      <td>-0.200222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.200222</td>\n",
       "      <td>-0.400443</td>\n",
       "      <td>0.132269</td>\n",
       "      <td>0.124225</td>\n",
       "      <td>0.256493</td>\n",
       "      <td>0.128247</td>\n",
       "      <td>-0.087042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.087042</td>\n",
       "      <td>-0.087042</td>\n",
       "      <td>-0.824212</td>\n",
       "      <td>0.977130</td>\n",
       "      <td>0.152918</td>\n",
       "      <td>0.076459</td>\n",
       "      <td>-0.035347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8591</th>\n",
       "      <td>7915</td>\n",
       "      <td>-0.215964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.215964</td>\n",
       "      <td>-0.431928</td>\n",
       "      <td>-0.130633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.130633</td>\n",
       "      <td>-0.065316</td>\n",
       "      <td>-0.045233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.045233</td>\n",
       "      <td>-0.045233</td>\n",
       "      <td>-0.058549</td>\n",
       "      <td>0.570356</td>\n",
       "      <td>0.511808</td>\n",
       "      <td>0.255904</td>\n",
       "      <td>-0.035822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8486</th>\n",
       "      <td>3896</td>\n",
       "      <td>-0.200784</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.200784</td>\n",
       "      <td>-0.401569</td>\n",
       "      <td>-0.055478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.055478</td>\n",
       "      <td>-0.027739</td>\n",
       "      <td>-0.093206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.093206</td>\n",
       "      <td>-0.093206</td>\n",
       "      <td>-0.115022</td>\n",
       "      <td>0.570356</td>\n",
       "      <td>0.455335</td>\n",
       "      <td>0.227667</td>\n",
       "      <td>-0.036856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8032</th>\n",
       "      <td>7393</td>\n",
       "      <td>-0.165926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.165926</td>\n",
       "      <td>-0.331851</td>\n",
       "      <td>0.069069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069069</td>\n",
       "      <td>0.034534</td>\n",
       "      <td>-0.041271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041271</td>\n",
       "      <td>-0.041271</td>\n",
       "      <td>-0.885183</td>\n",
       "      <td>0.971932</td>\n",
       "      <td>0.086749</td>\n",
       "      <td>0.043374</td>\n",
       "      <td>-0.036902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7844</th>\n",
       "      <td>3323</td>\n",
       "      <td>-0.148946</td>\n",
       "      <td>0.029881</td>\n",
       "      <td>-0.119065</td>\n",
       "      <td>-0.238130</td>\n",
       "      <td>-0.032348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.032348</td>\n",
       "      <td>-0.016174</td>\n",
       "      <td>-0.067770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.067770</td>\n",
       "      <td>-0.067770</td>\n",
       "      <td>-0.854441</td>\n",
       "      <td>0.892331</td>\n",
       "      <td>0.037889</td>\n",
       "      <td>0.018945</td>\n",
       "      <td>-0.037891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7398</th>\n",
       "      <td>6754</td>\n",
       "      <td>-0.120844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.120844</td>\n",
       "      <td>-0.241688</td>\n",
       "      <td>-0.032535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.032535</td>\n",
       "      <td>-0.016268</td>\n",
       "      <td>-0.057789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.057789</td>\n",
       "      <td>-0.057789</td>\n",
       "      <td>-0.957786</td>\n",
       "      <td>0.975692</td>\n",
       "      <td>0.017906</td>\n",
       "      <td>0.008953</td>\n",
       "      <td>-0.038349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8539</th>\n",
       "      <td>2883</td>\n",
       "      <td>-0.208169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.208169</td>\n",
       "      <td>-0.416339</td>\n",
       "      <td>0.078623</td>\n",
       "      <td>0.115443</td>\n",
       "      <td>0.194066</td>\n",
       "      <td>0.097033</td>\n",
       "      <td>-0.077346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.077346</td>\n",
       "      <td>-0.077346</td>\n",
       "      <td>-0.821491</td>\n",
       "      <td>0.993766</td>\n",
       "      <td>0.172275</td>\n",
       "      <td>0.086137</td>\n",
       "      <td>-0.038814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8292</th>\n",
       "      <td>598</td>\n",
       "      <td>-0.180422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.180422</td>\n",
       "      <td>-0.360845</td>\n",
       "      <td>0.023720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023720</td>\n",
       "      <td>0.011860</td>\n",
       "      <td>0.005624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005624</td>\n",
       "      <td>0.005624</td>\n",
       "      <td>-0.916592</td>\n",
       "      <td>0.973685</td>\n",
       "      <td>0.057093</td>\n",
       "      <td>0.028546</td>\n",
       "      <td>-0.039352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8692</th>\n",
       "      <td>1220</td>\n",
       "      <td>-0.241771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.241771</td>\n",
       "      <td>-0.483542</td>\n",
       "      <td>-0.085238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.085238</td>\n",
       "      <td>-0.042619</td>\n",
       "      <td>-0.130691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.130691</td>\n",
       "      <td>-0.130691</td>\n",
       "      <td>-0.260423</td>\n",
       "      <td>0.918263</td>\n",
       "      <td>0.657840</td>\n",
       "      <td>0.328920</td>\n",
       "      <td>-0.040991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7881</th>\n",
       "      <td>5930</td>\n",
       "      <td>-0.152199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.152199</td>\n",
       "      <td>-0.304398</td>\n",
       "      <td>-0.047736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.047736</td>\n",
       "      <td>-0.023868</td>\n",
       "      <td>-0.015793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.015793</td>\n",
       "      <td>-0.015793</td>\n",
       "      <td>-0.966475</td>\n",
       "      <td>0.979370</td>\n",
       "      <td>0.012895</td>\n",
       "      <td>0.006448</td>\n",
       "      <td>-0.042201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8594</th>\n",
       "      <td>5472</td>\n",
       "      <td>-0.216553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.216553</td>\n",
       "      <td>-0.433105</td>\n",
       "      <td>-0.078966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.078966</td>\n",
       "      <td>-0.039483</td>\n",
       "      <td>-0.080446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.080446</td>\n",
       "      <td>-0.080446</td>\n",
       "      <td>-0.142010</td>\n",
       "      <td>0.570356</td>\n",
       "      <td>0.428346</td>\n",
       "      <td>0.214173</td>\n",
       "      <td>-0.042358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8666</th>\n",
       "      <td>9839</td>\n",
       "      <td>-0.232506</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.232506</td>\n",
       "      <td>-0.465012</td>\n",
       "      <td>-0.081862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.081862</td>\n",
       "      <td>-0.040931</td>\n",
       "      <td>-0.107452</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.107452</td>\n",
       "      <td>-0.107452</td>\n",
       "      <td>-0.188056</td>\n",
       "      <td>0.727447</td>\n",
       "      <td>0.539391</td>\n",
       "      <td>0.269696</td>\n",
       "      <td>-0.042962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8681</th>\n",
       "      <td>7389</td>\n",
       "      <td>-0.236855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.236855</td>\n",
       "      <td>-0.473710</td>\n",
       "      <td>-0.160802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.160802</td>\n",
       "      <td>-0.080401</td>\n",
       "      <td>-0.089354</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.089354</td>\n",
       "      <td>-0.089354</td>\n",
       "      <td>0.196757</td>\n",
       "      <td>0.381463</td>\n",
       "      <td>0.578220</td>\n",
       "      <td>0.289110</td>\n",
       "      <td>-0.044294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8100</th>\n",
       "      <td>3229</td>\n",
       "      <td>-0.172361</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.172361</td>\n",
       "      <td>-0.344723</td>\n",
       "      <td>-0.007370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.007370</td>\n",
       "      <td>-0.003685</td>\n",
       "      <td>-0.020437</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.020437</td>\n",
       "      <td>-0.020437</td>\n",
       "      <td>-0.933681</td>\n",
       "      <td>0.957964</td>\n",
       "      <td>0.024283</td>\n",
       "      <td>0.012142</td>\n",
       "      <td>-0.044588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8529</th>\n",
       "      <td>3359</td>\n",
       "      <td>-0.205986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.205986</td>\n",
       "      <td>-0.411971</td>\n",
       "      <td>-0.147215</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.147215</td>\n",
       "      <td>-0.073607</td>\n",
       "      <td>-0.155999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.155999</td>\n",
       "      <td>-0.155999</td>\n",
       "      <td>-0.167297</td>\n",
       "      <td>0.570356</td>\n",
       "      <td>0.403060</td>\n",
       "      <td>0.201530</td>\n",
       "      <td>-0.055006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8472</th>\n",
       "      <td>2880</td>\n",
       "      <td>-0.198108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.198108</td>\n",
       "      <td>-0.396215</td>\n",
       "      <td>-0.006534</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.006534</td>\n",
       "      <td>-0.003267</td>\n",
       "      <td>-0.067716</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.067716</td>\n",
       "      <td>-0.067716</td>\n",
       "      <td>-0.986818</td>\n",
       "      <td>0.976334</td>\n",
       "      <td>-0.010483</td>\n",
       "      <td>-0.005242</td>\n",
       "      <td>-0.059055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8757</th>\n",
       "      <td>4571</td>\n",
       "      <td>-0.376943</td>\n",
       "      <td>0.092582</td>\n",
       "      <td>-0.284361</td>\n",
       "      <td>-0.568722</td>\n",
       "      <td>-0.080684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.080684</td>\n",
       "      <td>-0.040342</td>\n",
       "      <td>-0.102726</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.102726</td>\n",
       "      <td>-0.102726</td>\n",
       "      <td>-0.157535</td>\n",
       "      <td>0.570356</td>\n",
       "      <td>0.412822</td>\n",
       "      <td>0.206411</td>\n",
       "      <td>-0.063172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8753</th>\n",
       "      <td>8602</td>\n",
       "      <td>-0.321178</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.321178</td>\n",
       "      <td>-0.642356</td>\n",
       "      <td>-0.106164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.106164</td>\n",
       "      <td>-0.053082</td>\n",
       "      <td>-0.112912</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.112912</td>\n",
       "      <td>-0.112912</td>\n",
       "      <td>-0.044937</td>\n",
       "      <td>0.570356</td>\n",
       "      <td>0.525420</td>\n",
       "      <td>0.262710</td>\n",
       "      <td>-0.068205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8759 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_handle  svd_interest  cosine_interest  total_interest  \\\n",
       "132          691      0.350182         0.684712        1.034895   \n",
       "304         6251      0.281661         0.699234        0.980895   \n",
       "4           3200      0.725886         0.855162        1.581048   \n",
       "226         8146      0.306555         0.708142        1.014697   \n",
       "17          9561      0.524617         0.816094        1.340712   \n",
       "7            680      0.658800         0.886405        1.545205   \n",
       "0           3557      0.858217         0.915584        1.773800   \n",
       "67          2835      0.399411         0.690654        1.090065   \n",
       "11          6148      0.602835         0.793588        1.396423   \n",
       "2863        6147      0.060751         0.591608        0.652358   \n",
       "1           1968      0.841464         0.873326        1.714790   \n",
       "2           4391      0.763605         0.910259        1.673864   \n",
       "525           59      0.240150         0.445367        0.685517   \n",
       "1492        7045      0.138695         0.492805        0.631501   \n",
       "5           1712      0.691798         0.886547        1.578345   \n",
       "65          3310      0.401217         0.792825        1.194042   \n",
       "131         9665      0.351045         0.355117        0.706162   \n",
       "3           6531      0.732368         0.902378        1.634746   \n",
       "531         8260      0.239238         0.591301        0.830539   \n",
       "311         4316      0.280308         0.298347        0.578655   \n",
       "45          5576      0.429238         0.765320        1.194558   \n",
       "447         8472      0.255593         0.516398        0.771990   \n",
       "32           983      0.462450         0.665611        1.128061   \n",
       "807         9530      0.200952         0.668153        0.869105   \n",
       "19          8678      0.521813         0.620642        1.142456   \n",
       "23          3603      0.499646         0.708142        1.207788   \n",
       "41          5540      0.435326         0.762443        1.197769   \n",
       "661         4375      0.220919         0.478091        0.699010   \n",
       "887         7537      0.191582         0.478091        0.669673   \n",
       "6           8567      0.675377         0.768424        1.443802   \n",
       "...          ...           ...              ...             ...   \n",
       "6456        1993     -0.075590         0.000000       -0.075590   \n",
       "8691        7739     -0.240927         0.000000       -0.240927   \n",
       "8307        9362     -0.181539         0.000000       -0.181539   \n",
       "7091        9024     -0.103350         0.000000       -0.103350   \n",
       "8533        8480     -0.206605         0.000000       -0.206605   \n",
       "7792        9127     -0.145494         0.106904       -0.038589   \n",
       "7465        7143     -0.125601         0.000000       -0.125601   \n",
       "8285        7951     -0.179730         0.042258       -0.137472   \n",
       "8409        3247     -0.189661         0.000000       -0.189661   \n",
       "7209        9812     -0.110603         0.000000       -0.110603   \n",
       "8621        2203     -0.221767         0.000000       -0.221767   \n",
       "7636        5610     -0.135448         0.000000       -0.135448   \n",
       "8484        6337     -0.200222         0.000000       -0.200222   \n",
       "8591        7915     -0.215964         0.000000       -0.215964   \n",
       "8486        3896     -0.200784         0.000000       -0.200784   \n",
       "8032        7393     -0.165926         0.000000       -0.165926   \n",
       "7844        3323     -0.148946         0.029881       -0.119065   \n",
       "7398        6754     -0.120844         0.000000       -0.120844   \n",
       "8539        2883     -0.208169         0.000000       -0.208169   \n",
       "8292         598     -0.180422         0.000000       -0.180422   \n",
       "8692        1220     -0.241771         0.000000       -0.241771   \n",
       "7881        5930     -0.152199         0.000000       -0.152199   \n",
       "8594        5472     -0.216553         0.000000       -0.216553   \n",
       "8666        9839     -0.232506         0.000000       -0.232506   \n",
       "8681        7389     -0.236855         0.000000       -0.236855   \n",
       "8100        3229     -0.172361         0.000000       -0.172361   \n",
       "8529        3359     -0.205986         0.000000       -0.205986   \n",
       "8472        2880     -0.198108         0.000000       -0.198108   \n",
       "8757        4571     -0.376943         0.092582       -0.284361   \n",
       "8753        8602     -0.321178         0.000000       -0.321178   \n",
       "\n",
       "      weighted_AVG_interest  svd_course_tag  cosine_course_tag  \\\n",
       "132                2.069789        0.774473           0.714715   \n",
       "304                1.961790        0.774473           0.714715   \n",
       "4                  3.162097       -0.052290           0.148025   \n",
       "226                2.029393        0.650727           0.650709   \n",
       "17                 2.681423        0.269377           0.245916   \n",
       "7                  3.090410       -0.063299           0.000000   \n",
       "0                  3.547601       -0.168324           0.000000   \n",
       "67                 2.180131        0.344764           0.318295   \n",
       "11                 2.792846        0.033653           0.199804   \n",
       "2863               1.304717        0.774473           0.714715   \n",
       "1                  3.429579       -0.045452           0.000000   \n",
       "2                  3.347728        0.059635           0.191530   \n",
       "525                1.371035        0.774473           0.714715   \n",
       "1492               1.263001        0.774473           0.714715   \n",
       "5                  3.156690       -0.066466           0.000000   \n",
       "65                 2.388083        0.420977           0.341064   \n",
       "131                1.412325        0.761905           0.702056   \n",
       "3                  3.269491       -0.016901           0.000000   \n",
       "531                1.661078        0.485091           0.412373   \n",
       "311                1.157311        0.774473           0.714715   \n",
       "45                 2.389116        0.200894           0.298186   \n",
       "447                1.543981        0.635459           0.609879   \n",
       "32                 2.256123        0.440043           0.313650   \n",
       "807                1.738210        0.469814           0.361741   \n",
       "19                 2.284911        0.150282           0.124052   \n",
       "23                 2.415576        0.157076           0.197628   \n",
       "41                 2.395537        0.211454           0.260562   \n",
       "661                1.398021        0.518283           0.409214   \n",
       "887                1.339347        0.681331           0.585638   \n",
       "6                  2.887604       -0.074735           0.000000   \n",
       "...                     ...             ...                ...   \n",
       "6456              -0.151180       -0.048631           0.000000   \n",
       "8691              -0.481855       -0.088060           0.000000   \n",
       "8307              -0.363079       -0.079871           0.078956   \n",
       "7091              -0.206700       -0.121784           0.000000   \n",
       "8533              -0.413209       -0.204639           0.029966   \n",
       "7792              -0.077178       -0.120377           0.000000   \n",
       "7465              -0.251201       -0.029324           0.000000   \n",
       "8285              -0.274944        0.019186           0.096499   \n",
       "8409              -0.379323       -0.069126           0.000000   \n",
       "7209              -0.221206       -0.009344           0.000000   \n",
       "8621              -0.443534       -0.033929           0.000000   \n",
       "7636              -0.270896       -0.011489           0.047399   \n",
       "8484              -0.400443        0.132269           0.124225   \n",
       "8591              -0.431928       -0.130633           0.000000   \n",
       "8486              -0.401569       -0.055478           0.000000   \n",
       "8032              -0.331851        0.069069           0.000000   \n",
       "7844              -0.238130       -0.032348           0.000000   \n",
       "7398              -0.241688       -0.032535           0.000000   \n",
       "8539              -0.416339        0.078623           0.115443   \n",
       "8292              -0.360845        0.023720           0.000000   \n",
       "8692              -0.483542       -0.085238           0.000000   \n",
       "7881              -0.304398       -0.047736           0.000000   \n",
       "8594              -0.433105       -0.078966           0.000000   \n",
       "8666              -0.465012       -0.081862           0.000000   \n",
       "8681              -0.473710       -0.160802           0.000000   \n",
       "8100              -0.344723       -0.007370           0.000000   \n",
       "8529              -0.411971       -0.147215           0.000000   \n",
       "8472              -0.396215       -0.006534           0.000000   \n",
       "8757              -0.568722       -0.080684           0.000000   \n",
       "8753              -0.642356       -0.106164           0.000000   \n",
       "\n",
       "      total_course_tag  weighted_AVG_course_tag  svd_course_view  \\\n",
       "132           1.489188                 0.744594         0.661832   \n",
       "304           1.489188                 0.744594         0.661832   \n",
       "4             0.095735                 0.047867         0.003532   \n",
       "226           1.301435                 0.650718         0.481604   \n",
       "17            0.515292                 0.257646         0.171234   \n",
       "7            -0.063299                -0.031650        -0.018236   \n",
       "0            -0.168324                -0.084162         0.011961   \n",
       "67            0.663060                 0.331530         0.296326   \n",
       "11            0.233457                 0.116728         0.222695   \n",
       "2863          1.489188                 0.744594         0.661832   \n",
       "1            -0.045452                -0.022726        -0.031499   \n",
       "2             0.251166                 0.125583         0.056307   \n",
       "525           1.489188                 0.744594         0.661832   \n",
       "1492          1.489188                 0.744594         0.661832   \n",
       "5            -0.066466                -0.033233         0.005688   \n",
       "65            0.762041                 0.381020         0.225043   \n",
       "131           1.463961                 0.731980         0.615043   \n",
       "3            -0.016901                -0.008450        -0.029037   \n",
       "531           0.897464                 0.448732         0.567814   \n",
       "311           1.489188                 0.744594         0.661832   \n",
       "45            0.499080                 0.249540         0.205854   \n",
       "447           1.245338                 0.622669         0.559034   \n",
       "32            0.753693                 0.376846         0.247503   \n",
       "807           0.831556                 0.415778         0.401836   \n",
       "19            0.274334                 0.137167         0.310178   \n",
       "23            0.354704                 0.177352         0.086966   \n",
       "41            0.472017                 0.236008         0.089816   \n",
       "661           0.927497                 0.463748         0.606484   \n",
       "887           1.266969                 0.633484         0.603174   \n",
       "6            -0.074735                -0.037367         0.002929   \n",
       "...                ...                      ...              ...   \n",
       "6456         -0.048631                -0.024316        -0.032030   \n",
       "8691         -0.088060                -0.044030         0.006801   \n",
       "8307         -0.000914                -0.000457        -0.050199   \n",
       "7091         -0.121784                -0.060892         0.013180   \n",
       "8533         -0.174673                -0.087337        -0.117066   \n",
       "7792         -0.120377                -0.060189        -0.090801   \n",
       "7465         -0.029324                -0.014662         0.013120   \n",
       "8285          0.115685                 0.057843        -0.091558   \n",
       "8409         -0.069126                -0.034563        -0.074356   \n",
       "7209         -0.009344                -0.004672        -0.010603   \n",
       "8621         -0.033929                -0.016964        -0.044119   \n",
       "7636          0.035910                 0.017955        -0.023226   \n",
       "8484          0.256493                 0.128247        -0.087042   \n",
       "8591         -0.130633                -0.065316        -0.045233   \n",
       "8486         -0.055478                -0.027739        -0.093206   \n",
       "8032          0.069069                 0.034534        -0.041271   \n",
       "7844         -0.032348                -0.016174        -0.067770   \n",
       "7398         -0.032535                -0.016268        -0.057789   \n",
       "8539          0.194066                 0.097033        -0.077346   \n",
       "8292          0.023720                 0.011860         0.005624   \n",
       "8692         -0.085238                -0.042619        -0.130691   \n",
       "7881         -0.047736                -0.023868        -0.015793   \n",
       "8594         -0.078966                -0.039483        -0.080446   \n",
       "8666         -0.081862                -0.040931        -0.107452   \n",
       "8681         -0.160802                -0.080401        -0.089354   \n",
       "8100         -0.007370                -0.003685        -0.020437   \n",
       "8529         -0.147215                -0.073607        -0.155999   \n",
       "8472         -0.006534                -0.003267        -0.067716   \n",
       "8757         -0.080684                -0.040342        -0.102726   \n",
       "8753         -0.106164                -0.053082        -0.112912   \n",
       "\n",
       "      cosine_course_view  total_course_view  weighted_AVG_course_view  \\\n",
       "132             0.505936           1.167768                  1.167768   \n",
       "304             0.505936           1.167768                  1.167768   \n",
       "4               0.058832           0.062364                  0.062364   \n",
       "226             0.369292           0.850896                  0.850896   \n",
       "17              0.143115           0.314350                  0.314350   \n",
       "7               0.000000          -0.018236                 -0.018236   \n",
       "0               0.000000           0.011961                  0.011961   \n",
       "67              0.179813           0.476139                  0.476139   \n",
       "11              0.096576           0.319271                  0.319271   \n",
       "2863            0.505936           1.167768                  1.167768   \n",
       "1               0.000000          -0.031499                 -0.031499   \n",
       "2               0.057330           0.113637                  0.113637   \n",
       "525             0.505936           1.167768                  1.167768   \n",
       "1492            0.505936           1.167768                  1.167768   \n",
       "5               0.000000           0.005688                  0.005688   \n",
       "65              0.180720           0.405763                  0.405763   \n",
       "131             0.255716           0.870759                  0.870759   \n",
       "3               0.000000          -0.029037                 -0.029037   \n",
       "531             0.337980           0.905794                  0.905794   \n",
       "311             0.505936           1.167768                  1.167768   \n",
       "45              0.134537           0.340391                  0.340391   \n",
       "447             0.382814           0.941848                  0.941848   \n",
       "32              0.175440           0.422943                  0.422943   \n",
       "807             0.257348           0.659184                  0.659184   \n",
       "19              0.110196           0.420374                  0.420374   \n",
       "23              0.088251           0.175217                  0.175217   \n",
       "41              0.156071           0.245887                  0.245887   \n",
       "661             0.289676           0.896160                  0.896160   \n",
       "887             0.439321           1.042495                  1.042495   \n",
       "6               0.000000           0.002929                  0.002929   \n",
       "...                  ...                ...                       ...   \n",
       "6456            0.000000          -0.032030                 -0.032030   \n",
       "8691            0.000000           0.006801                  0.006801   \n",
       "8307            0.000000          -0.050199                 -0.050199   \n",
       "7091            0.000000           0.013180                  0.013180   \n",
       "8533            0.000000          -0.117066                 -0.117066   \n",
       "7792            0.000000          -0.090801                 -0.090801   \n",
       "7465            0.000000           0.013120                  0.013120   \n",
       "8285            0.000000          -0.091558                 -0.091558   \n",
       "8409            0.000000          -0.074356                 -0.074356   \n",
       "7209            0.000000          -0.010603                 -0.010603   \n",
       "8621            0.000000          -0.044119                 -0.044119   \n",
       "7636            0.000000          -0.023226                 -0.023226   \n",
       "8484            0.000000          -0.087042                 -0.087042   \n",
       "8591            0.000000          -0.045233                 -0.045233   \n",
       "8486            0.000000          -0.093206                 -0.093206   \n",
       "8032            0.000000          -0.041271                 -0.041271   \n",
       "7844            0.000000          -0.067770                 -0.067770   \n",
       "7398            0.000000          -0.057789                 -0.057789   \n",
       "8539            0.000000          -0.077346                 -0.077346   \n",
       "8292            0.000000           0.005624                  0.005624   \n",
       "8692            0.000000          -0.130691                 -0.130691   \n",
       "7881            0.000000          -0.015793                 -0.015793   \n",
       "8594            0.000000          -0.080446                 -0.080446   \n",
       "8666            0.000000          -0.107452                 -0.107452   \n",
       "8681            0.000000          -0.089354                 -0.089354   \n",
       "8100            0.000000          -0.020437                 -0.020437   \n",
       "8529            0.000000          -0.155999                 -0.155999   \n",
       "8472            0.000000          -0.067716                 -0.067716   \n",
       "8757            0.000000          -0.102726                 -0.102726   \n",
       "8753            0.000000          -0.112912                 -0.112912   \n",
       "\n",
       "      svd_course_level  cosine_course_level  total_course_level  \\\n",
       "132           0.127815             0.727447            0.855262   \n",
       "304           0.127815             0.727447            0.855262   \n",
       "4             0.832783             0.989153            1.821936   \n",
       "226           0.290758             0.979237            1.269994   \n",
       "17            0.651999             0.976047            1.628045   \n",
       "7             0.958799             0.950850            1.909649   \n",
       "0             0.158436             0.727447            0.885883   \n",
       "67            0.459806             0.990342            1.450148   \n",
       "11           -0.011053             0.965860            0.954808   \n",
       "2863          0.205153             0.727447            0.932600   \n",
       "1             0.042376             0.570356            0.612732   \n",
       "2            -0.807918             0.929234            0.121316   \n",
       "525          -0.004212             0.727447            0.723235   \n",
       "1492          0.127815             0.727447            0.855262   \n",
       "5             0.190477             0.727447            0.917925   \n",
       "65           -0.154091             0.918260            0.764169   \n",
       "131           0.196332             0.859816            1.056148   \n",
       "3            -0.121468             0.727447            0.605979   \n",
       "531           0.046818             0.923191            0.970008   \n",
       "311           0.127815             0.727447            0.855262   \n",
       "45            0.004174             0.920608            0.924782   \n",
       "447          -0.290231             0.923333            0.633102   \n",
       "32           -0.182685             0.916216            0.733531   \n",
       "807           0.261607             0.913943            1.175550   \n",
       "19            0.164827             0.924237            1.089064   \n",
       "23            0.295123             0.920526            1.215650   \n",
       "41            0.058427             0.924041            0.982467   \n",
       "661           0.282763             0.913599            1.196362   \n",
       "887          -0.060262             0.727447            0.667185   \n",
       "6             0.066851             0.921535            0.988386   \n",
       "...                ...                  ...                 ...   \n",
       "6456         -0.988856             0.959473           -0.029383   \n",
       "8691         -0.134208             0.727447            0.593240   \n",
       "8307         -0.604570             0.983832            0.379262   \n",
       "7091         -0.925436             0.981083            0.055647   \n",
       "8533         -0.157948             0.922817            0.764869   \n",
       "7792         -0.968116             0.952961           -0.015155   \n",
       "7465         -0.928441             0.960413            0.031972   \n",
       "8285         -0.835416             0.978365            0.142948   \n",
       "8409         -0.229392             0.727447            0.498055   \n",
       "7209         -0.998010             0.971940           -0.026071   \n",
       "8621         -0.494157             0.997890            0.503733   \n",
       "7636         -0.975229             0.968037           -0.007192   \n",
       "8484         -0.824212             0.977130            0.152918   \n",
       "8591         -0.058549             0.570356            0.511808   \n",
       "8486         -0.115022             0.570356            0.455335   \n",
       "8032         -0.885183             0.971932            0.086749   \n",
       "7844         -0.854441             0.892331            0.037889   \n",
       "7398         -0.957786             0.975692            0.017906   \n",
       "8539         -0.821491             0.993766            0.172275   \n",
       "8292         -0.916592             0.973685            0.057093   \n",
       "8692         -0.260423             0.918263            0.657840   \n",
       "7881         -0.966475             0.979370            0.012895   \n",
       "8594         -0.142010             0.570356            0.428346   \n",
       "8666         -0.188056             0.727447            0.539391   \n",
       "8681          0.196757             0.381463            0.578220   \n",
       "8100         -0.933681             0.957964            0.024283   \n",
       "8529         -0.167297             0.570356            0.403060   \n",
       "8472         -0.986818             0.976334           -0.010483   \n",
       "8757         -0.157535             0.570356            0.412822   \n",
       "8753         -0.044937             0.570356            0.525420   \n",
       "\n",
       "      weighted_AVG_course_level  AVG_OF_ALL  \n",
       "132                    0.427631    0.551223  \n",
       "304                    0.427631    0.537723  \n",
       "4                      0.910968    0.522912  \n",
       "226                    0.634997    0.520750  \n",
       "17                     0.814023    0.508430  \n",
       "7                      0.954825    0.499419  \n",
       "0                      0.442942    0.489793  \n",
       "67                     0.725074    0.464109  \n",
       "11                     0.477404    0.463281  \n",
       "2863                   0.466300    0.460422  \n",
       "1                      0.306366    0.460215  \n",
       "2                      0.060658    0.455951  \n",
       "525                    0.361617    0.455627  \n",
       "1492                   0.427631    0.450374  \n",
       "5                      0.458962    0.448513  \n",
       "65                     0.382084    0.444619  \n",
       "131                    0.528074    0.442892  \n",
       "3                      0.302990    0.441874  \n",
       "531                    0.485004    0.437576  \n",
       "311                    0.427631    0.437163  \n",
       "45                     0.462391    0.430180  \n",
       "447                    0.316551    0.428131  \n",
       "32                     0.366765    0.427835  \n",
       "807                    0.587775    0.425118  \n",
       "19                     0.544532    0.423373  \n",
       "23                     0.607825    0.421996  \n",
       "41                     0.491234    0.421083  \n",
       "661                    0.598181    0.419514  \n",
       "887                    0.333593    0.418615  \n",
       "6                      0.494193    0.418420  \n",
       "...                         ...         ...  \n",
       "6456                  -0.014691   -0.027777  \n",
       "8691                   0.296620   -0.027808  \n",
       "8307                   0.189631   -0.028013  \n",
       "7091                   0.027823   -0.028324  \n",
       "8533                   0.382435   -0.029397  \n",
       "7792                  -0.007577   -0.029468  \n",
       "7465                   0.015986   -0.029595  \n",
       "8285                   0.071474   -0.029648  \n",
       "8409                   0.249027   -0.029902  \n",
       "7209                  -0.013035   -0.031190  \n",
       "8621                   0.251867   -0.031594  \n",
       "7636                  -0.003596   -0.034970  \n",
       "8484                   0.076459   -0.035347  \n",
       "8591                   0.255904   -0.035822  \n",
       "8486                   0.227667   -0.036856  \n",
       "8032                   0.043374   -0.036902  \n",
       "7844                   0.018945   -0.037891  \n",
       "7398                   0.008953   -0.038349  \n",
       "8539                   0.086137   -0.038814  \n",
       "8292                   0.028546   -0.039352  \n",
       "8692                   0.328920   -0.040991  \n",
       "7881                   0.006448   -0.042201  \n",
       "8594                   0.214173   -0.042358  \n",
       "8666                   0.269696   -0.042962  \n",
       "8681                   0.289110   -0.044294  \n",
       "8100                   0.012142   -0.044588  \n",
       "8529                   0.201530   -0.055006  \n",
       "8472                  -0.005242   -0.059055  \n",
       "8757                   0.206411   -0.063172  \n",
       "8753                   0.262710   -0.068205  \n",
       "\n",
       "[8759 rows x 18 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_measures = {\n",
    "    'user_assessment_similarity_measure' : user_assessment_similarity_measure,\n",
    "    'user_interest_similarity_measure' : user_interest_similarity_measure,\n",
    "    'user_courseview_tag_similarity_measure' : user_courseview_tag_similarity_measure,\n",
    "    'user_courseview_similarity_measure' : user_courseview_similarity_measure,\n",
    "    'user_course_level_similarity_measure' : user_course_level_similarity_measure,\n",
    "    'score_weights' : {'A' : 1, 'I' : 1, 'CVT' : 1, 'CV' : 1, 'CL' : 1}\n",
    "}\n",
    "similar_users_model = SimiliarUsers(similarity_measures)\n",
    "similar_users_model.set_score_weights({'A' : 4, 'I' : 4, 'CVT' : 1, 'CV' : 2, 'CL' : 1})\n",
    "similar_users_df = similar_users_model.get_similar_users('1')\n",
    "print(similar_users_df.columns)\n",
    "similar_users_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_weights = {'A' : 1, 'I' : 8, 'CVT' : 0, 'CV' : 0, 'CL' : 0}\n",
    "sum(score_weights.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_handle</th>\n",
       "      <th>interest_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>mvc-scaffolding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>mvc2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>mvc-html-helpers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>mvc4-ioc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>mvc-testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>mvc3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>mvc5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>mvc4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>asp.net-mvc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>mvc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>angular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>javascript-frameworks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>javascript-libraries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>javascript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>c#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>java</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>asp.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>react.js</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>asp.net-core</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>html5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>ice-cream-sandwich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>ef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>entity-framework</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>powershell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>powershell-v3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>sharepoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>data-analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1</td>\n",
       "      <td>cpp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>1</td>\n",
       "      <td>spring-mvc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>1</td>\n",
       "      <td>sprig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1</td>\n",
       "      <td>spring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1</td>\n",
       "      <td>css</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>1</td>\n",
       "      <td>jquery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1</td>\n",
       "      <td>restful-api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>1</td>\n",
       "      <td>rest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1</td>\n",
       "      <td>git</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>1</td>\n",
       "      <td>azure-mobile-services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>1</td>\n",
       "      <td>azure-deployment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>1</td>\n",
       "      <td>azure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>1</td>\n",
       "      <td>microsoft-azure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1</td>\n",
       "      <td>windows-azure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>1</td>\n",
       "      <td>ios8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>1</td>\n",
       "      <td>ios7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>1</td>\n",
       "      <td>ios</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>1</td>\n",
       "      <td>async</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>1</td>\n",
       "      <td>xamarin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1</td>\n",
       "      <td>tdd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1</td>\n",
       "      <td>test-driven-development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1</td>\n",
       "      <td>architecture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>1</td>\n",
       "      <td>business-intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>1</td>\n",
       "      <td>aws-databases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1</td>\n",
       "      <td>aws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>1</td>\n",
       "      <td>linq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>1</td>\n",
       "      <td>sql</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>1</td>\n",
       "      <td>software-practices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>1</td>\n",
       "      <td>machine-learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1</td>\n",
       "      <td>bootstrap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_handle             interest_tag\n",
       "0             1          mvc-scaffolding\n",
       "1             1                     mvc2\n",
       "2             1         mvc-html-helpers\n",
       "3             1                 mvc4-ioc\n",
       "4             1              mvc-testing\n",
       "5             1                     mvc3\n",
       "6             1                     mvc5\n",
       "7             1                     mvc4\n",
       "8             1              asp.net-mvc\n",
       "9             1                      mvc\n",
       "10            1                  angular\n",
       "11            1    javascript-frameworks\n",
       "12            1     javascript-libraries\n",
       "13            1               javascript\n",
       "14            1                       c#\n",
       "15            1                     java\n",
       "16            1                  asp.net\n",
       "17            1                 react.js\n",
       "18            1                     .net\n",
       "19            1             asp.net-core\n",
       "20            1                    html5\n",
       "21            1       ice-cream-sandwich\n",
       "22            1                  android\n",
       "23            1                       ef\n",
       "24            1         entity-framework\n",
       "25            1               powershell\n",
       "26            1            powershell-v3\n",
       "27            1               sharepoint\n",
       "28            1            data-analysis\n",
       "29            1                   python\n",
       "..          ...                      ...\n",
       "110           1                      cpp\n",
       "111           1               spring-mvc\n",
       "112           1                    sprig\n",
       "113           1                   spring\n",
       "114           1                      css\n",
       "115           1                   jquery\n",
       "116           1              restful-api\n",
       "117           1                     rest\n",
       "118           1                      git\n",
       "119           1    azure-mobile-services\n",
       "120           1         azure-deployment\n",
       "121           1                    azure\n",
       "122           1          microsoft-azure\n",
       "123           1            windows-azure\n",
       "124           1                     ios8\n",
       "125           1                     ios7\n",
       "126           1                      ios\n",
       "127           1                    async\n",
       "128           1                  xamarin\n",
       "129           1                      tdd\n",
       "130           1  test-driven-development\n",
       "131           1             architecture\n",
       "132           1    business-intelligence\n",
       "133           1            aws-databases\n",
       "134           1                      aws\n",
       "135           1                     linq\n",
       "136           1                      sql\n",
       "137           1       software-practices\n",
       "138           1         machine-learning\n",
       "139           1                bootstrap\n",
       "\n",
       "[140 rows x 2 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = '1'\n",
    "user_assessment_scores[user_assessment_scores.user_handle == ref][['user_handle', 'assessment_tag', \n",
    "                                                                   'user_assessment_score']]\n",
    "user_interests[user_interests.user_handle == ref][['user_handle', 'interest_tag']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deployment :"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
